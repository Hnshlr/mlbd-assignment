{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MLBD : Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TODO: Switch methods from panda to spark when possible\n",
    "### TODO: Work on a way to acquire data multiples times a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import *\n",
    "import urllib.request, json, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spark session builder:\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Locally save instances of the data:\n",
    "import time\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "# sleep until specific time (e.g. 5pm) before running the next line\n",
    "today = datetime.datetime.now()\n",
    "exactImportTime = datetime.datetime(today.year, today.month, today.day, 17, 0, 0)\n",
    "awaitingTime = exactImportTime - today\n",
    "time.sleep(awaitingTime.total_seconds())\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "with urllib.request.urlopen(url5min) as url:\n",
    "    data5min = json.load(url)\n",
    "with open('output/5min/data5min_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "    json.dump(data5min, outfile)\n",
    "with urllib.request.urlopen(url24h) as url:\n",
    "    data24h = json.load(url)\n",
    "with open('output/24h/data24h_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "    json.dump(data24h, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from local files and load them into Spark DataFrames:\n",
    "file1 = 'output/data24h_2022-10-31.json'\n",
    "file2 = 'output/data24h_2022-11-01.json'\n",
    "\n",
    "# File 1:\n",
    "spark.sparkContext.addFile(file1)\n",
    "filename_1 = SparkFiles.get(file1.split('/')[-1])\n",
    "df1 = spark.read.json(filename_1)\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df1.printSchema()\n",
    "\n",
    "# File 2:\n",
    "spark.sparkContext.addFile(file2)\n",
    "filename_2 = SparkFiles.get(file2.split('/')[-1])\n",
    "df2 = spark.read.json(filename_2)\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spark dataframes used for tasks :\n",
    "dfs = [df1, df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# AQI Map:\n",
    "air = {}\n",
    "air[1] = [\"Low\", [0,16], [0,11]]\n",
    "air[2] = [\"Low\", [17,33], [12,23]]\n",
    "air[3] = [\"Low\", [34,50], [24,35]]\n",
    "air[4] = [\"Medium\", [51,58], [36,41]]\n",
    "air[5] = [\"Medium\", [59,66], [42,47]]\n",
    "air[6] = [\"Medium\", [67,75], [48,53]]\n",
    "air[7] = [\"High\", [76,83], [54,58]]\n",
    "air[8] = [\"High\", [84,91], [59,64]]\n",
    "air[9] = [\"High\", [92,100], [65,70]]\n",
    "air[10] = [\"Very High\", [101,10000000], [71,10000000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark implementation & tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing (P1 and P2 filtering):\n",
    "for i in range(len(dfs)):\n",
    "    print(\"Raw dataset count: \", dfs[i].count())\n",
    "    # Explode sensordatavalues using pyspark.sql.functions.explode\n",
    "    df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    print(\"Dataset count after exploding sensordatavalues: \", df_.count())\n",
    "    # Remove rows that aren't P1 or P2:\n",
    "    df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "    print(\"Dataset count after removing rows that aren't P1 or P2: \", df_.count())\n",
    "    # Regroup sensordatavalues by record id:\n",
    "    df_ = df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Remove the old sensordatavalues column still containing values different from P1 and P2:\n",
    "    dfs[i] = dfs[i].drop('sensordatavalues')\n",
    "    # Link the new sensordatavalues column to the old dataframe, on id:\n",
    "    dfs[i] = dfs[i].join(df_, on='id', how='inner')\n",
    "    print(\"Dataset count after joining the new sensordatavalues column to the old dataframe: \", dfs[i].count())\n",
    "\n",
    "    dfs[i].show(5)\n",
    "    # dfs[i].select('id','location.country','location.id', 'sensordatavalues.value_type','sensordatavalues.value').sort('location.country', 'location.id').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    # replace location field with country field:\n",
    "    df_ = dfs[i].withColumn('location', dfs[i]['location.country'])\n",
    "    # sort by country:\n",
    "    df_ = df_.sort('location')\n",
    "    # explode sensordatavalues:\n",
    "    df_ = df_.withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    # group by country:\n",
    "    df_ = df_.groupby('location').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Create a RDD collection of tuples (country, (P1, P2)), so that each rdd element is a combo of a country and either its P1 or P2 values. Then convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [float(y['value']) for y in x[1] if y['value_type'] == 'P1'])).toDF(['location', 'P1']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [float(y['value']) for y in x[1] if y['value_type'] == 'P2'])).toDF(['location', 'P2']), on='location', how='inner')\n",
    "    # Create a RDD collection to calculate the average P1 and P2 values for each country, and convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], float(np.round(np.mean(x[2]), 2)))).toDF(['location', 'avgP1']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], float(np.round(np.mean(x[3]), 2)))).toDF(['location', 'avgP1']), on='location', how='inner')\n",
    "    # Associate P1 and P2 avg to their respective AQI:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[4]) <= air[y][2][1]][0])).toDF(['location', 'P1_AQI']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[5]) <= air[y][2][1]][0])).toDF(['location', 'P2_AQI']), on='location', how='inner')\n",
    "    # Calculate the max AQI for each country:\n",
    "    df_ = df_.withColumn('maxAQI', when(df_.P1_AQI > df_.P2_AQI, df_.P1_AQI).otherwise(df_.P2_AQI))\n",
    "\n",
    "    dfs[i] = df_\n",
    "    dfs[i].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country.\n",
    "df1 = dfs[0].select('location', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "df2 = dfs[1].select('location', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "df_diff = df1.join(df2, on='location', how='inner')\n",
    "df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('location', 'diffAQI')\n",
    "df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "df_diff.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Using the geo-coordinates from the sensor data, group the data into smaller regions using an appropriate clustering algorithm. Then determine the top 50 regions in terms of air quality improvement over the previous 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "dfs = [df1, df2]\n",
    "for i in range(len(dfs)):\n",
    "    # Same process as task 1, we filter the dataframe to keep only P1 and P2 values\n",
    "    df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "    dfs[i] = dfs[i].drop('sensordatavalues').join(df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues')), on='id', how='inner')\n",
    "    dfs[i] = dfs[i].select('sensor.id','location.latitude','location.longitude', 'location.altitude', 'sensordatavalues').sort('sensor.id')\n",
    "\n",
    "# Create a RDD with the sensor id, and a tuple of the latitude and longitude, using the first dataframe:\n",
    "rdd = dfs[0].rdd.map(lambda x: (x[0], (float(x[1]), float(x[2]))))\n",
    "# Create a KMeans model with 200 clusters using latitude and longitude values from the first dataframe:\n",
    "model = KMeans.train(rdd.map(lambda x: x[1]), 200, maxIterations=10, initializationMode=\"k-means||\", seed=23)\n",
    "# (This model will be used to predict the cluster for each sensor id in the second dataframe.)\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    # Create a RDD with the sensor id, and a tuple of the latitude and longitude using the current dataframe:\n",
    "    rdd = dfs[i].rdd.map(lambda x: (x[0], (float(x[1]), float(x[2]))))\n",
    "    # Create a RDD collection with both the sensor id and the corresponding predicted cluster:\n",
    "    rdd_clusters = rdd.map(lambda x: (x[0], model.predict(x[1])))\n",
    "    # Store each cluster center in a dictionary (Each cluster center is a tuple of latitude and longitude), rounded to 2 decimals:\n",
    "    centers = {i: np.round(np.array(model.clusterCenters[i]), 2) for i in range(len(model.clusterCenters))}\n",
    "    # Create a RDD collection with the cluster id, the sensor id, and the cluster center:\n",
    "    rdd_clusters = rdd_clusters.map(lambda x: (x[1], x[0], centers[x[1]].tolist()))\n",
    "    # Convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(rdd_clusters.toDF(['cluster_id', 'id', 'cluster_center']), on='id', how='inner')\n",
    "    # Group by cluster, keeping the cluster center and the sensordatavalues:\n",
    "    dfs[i] = dfs[i].groupby('cluster_id').agg(collect_list('cluster_center')[0].alias('cluster_center'), collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # PROBLEM: sensordatavalues is a list of list of data values. To fix this, explode two times the sensordatavalues column:\n",
    "    dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    # Then regroup by cluster, keeping the cluster center and the sensordatavalues for each cluster id:\n",
    "    dfs[i] = dfs[i].groupby('cluster_id').agg(collect_list('cluster_center')[0].alias('cluster_center'), collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Once again, just like task 1, we compute the max AQI for each cluster:\n",
    "    # Create a RDD collection of tuples (country, (P1, P2)), so that each rdd element is a combo of a cluster and eOk ither its P1 or P2 values. Then convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [float(y['value']) for y in x[2] if y[2] == 'P1'])).toDF(['cluster_id', 'P1']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [float(y['value']) for y in x[2] if y[2] == 'P2'])).toDF(['cluster_id', 'P2']), on='cluster_id', how='inner')\n",
    "    # Create a RDD collection to calculate the average P1 and P2 values for each cluster, and convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], float(np.round(np.mean(x[3]), 2)))).toDF(['cluster_id', 'avgP1']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], float(np.round(np.mean(x[4]), 2)))).toDF(['cluster_id', 'avgP1']), on='cluster_id', how='inner')\n",
    "    # Associate P1 and P2 avg to their respective AQI:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[5]) <= air[y][2][1]][0])).toDF(['cluster_id', 'P1_AQI']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[6]) <= air[y][2][1]][0])).toDF(['cluster_id', 'P2_AQI']), on='cluster_id', how='inner')\n",
    "    # Calculate the max AQI for each country:\n",
    "    dfs[i] = dfs[i].withColumn('maxAQI', when(dfs[i].P1_AQI > dfs[i].P2_AQI, dfs[i].P1_AQI).otherwise(dfs[i].P2_AQI))\n",
    "    dfs[i] = dfs[i].select('cluster_id', 'cluster_center', 'maxAQI')\n",
    "    dfs[i].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df1 = dfs[0].select('cluster_id', 'cluster_center', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "df2 = dfs[1].select('cluster_id', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "df_diff = df1.join(df2, on='cluster_id', how='inner')\n",
    "df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1)\n",
    "df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "df_diff.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
