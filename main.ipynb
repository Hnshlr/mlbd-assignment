{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MSc in CSTE, CIDA option Machine learning & Big Data Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Analysis of data from an environmental sensor network using Hadoop/Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import *\n",
    "import urllib.request, json, datetime, time\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from itertools import groupby\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Allocate more memory to the driver:\n",
    "MAX_MEMORY= \"8g\"\n",
    "# Spark session builder:\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\", MAX_MEMORY).config(\"spark.driver.memory\", MAX_MEMORY).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"OFF\")\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Locally save instances of the data:\n",
    "def save_data(wait):\n",
    "    # URLs of the data:\n",
    "    url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "    url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "    # If wait=True, sleep until specific time (e.g. 5pm) before running the next line\n",
    "    if wait:\n",
    "        today = datetime.datetime.now()\n",
    "        exactImportTime = datetime.datetime(today.year, today.month, today.day, 17, 0, 0)\n",
    "        awaitingTime = exactImportTime - today\n",
    "        time.sleep(awaitingTime.total_seconds())\n",
    "\n",
    "    # Download the data, saved as json files:\n",
    "    today = datetime.datetime.now()\n",
    "    with urllib.request.urlopen(url5min) as url:\n",
    "        data5min = json.load(url)\n",
    "    with open('output/5min/data5min_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "        json.dump(data5min, outfile)\n",
    "    with urllib.request.urlopen(url24h) as url:\n",
    "        data24h = json.load(url)\n",
    "    with open('output/24h/data24h_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "        json.dump(data24h, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from local files and load them into Spark DataFrames:\n",
    "def load_data(all):\n",
    "    path = 'output/24h/'\n",
    "    file1 = 'data24h_2022-11-3_17h00.json'\n",
    "    file2 = 'data24h_2022-11-4_17h00.json'\n",
    "    file3 = 'data24h_2022-11-5_17h00.json'\n",
    "    file4 = 'data24h_2022-11-6_17h00.json'\n",
    "    file5 = 'data24h_2022-11-7_17h00.json'\n",
    "    file6 = 'data24h_2022-11-8_17h00.json'\n",
    "    file7 = 'data24h_2022-11-9_17h00.json'\n",
    "    file8 = 'data24h_2022-11-10_17h00.json'\n",
    "    file9 = 'data24h_2022-11-11_17h00.json'\n",
    "    file10 = 'data24h_2022-11-12_17h00.json'\n",
    "    file11 = 'data24h_2022-11-13_17h00.json'\n",
    "\n",
    "    files = [file1, file2, file3, file4, file5, file6, file7, file8, file9, file10, file11]\n",
    "    dfs = []\n",
    "\n",
    "    if all:\n",
    "        for file in files:\n",
    "            spark.sparkContext.addFile(path + file)\n",
    "            filename = SparkFiles.get(file)\n",
    "            df = spark.read.json(filename)\n",
    "            dfs.append(df)\n",
    "    else:\n",
    "        for file in files[len(files)-2:]:\n",
    "            spark.sparkContext.addFile(path + file)\n",
    "            filename = SparkFiles.get(file)\n",
    "            df = spark.read.json(filename)\n",
    "            dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the AQI Map:\n",
    "def air_quality_map():\n",
    "    air = {}\n",
    "    air[1] = [\"Low\", [0,16], [0,11]]\n",
    "    air[2] = [\"Low\", [17,33], [12,23]]\n",
    "    air[3] = [\"Low\", [34,50], [24,35]]\n",
    "    air[4] = [\"Medium\", [51,58], [36,41]]\n",
    "    air[5] = [\"Medium\", [59,66], [42,47]]\n",
    "    air[6] = [\"Medium\", [67,75], [48,53]]\n",
    "    air[7] = [\"High\", [76,83], [54,58]]\n",
    "    air[8] = [\"High\", [84,91], [59,64]]\n",
    "    air[9] = [\"High\", [92,100], [65,70]]\n",
    "    air[10] = [\"Very High\", [101,10000000], [71,10000000]]\n",
    "    return air"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark implementation & tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 0: Data pre-processing, filtering and cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing (P1 and P2 filtering) (Common to all tasks, the returned dataframe will be used for all three tasks):\n",
    "def preprocessing(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        # Remove columns that are not needed for all three tasks:\n",
    "        dfs[i] = dfs[i].drop('sampling_rate', 'timestamp').withColumn('country', dfs[i].location.country).withColumn('latitude', dfs[i].location.latitude).withColumn('longitude', dfs[i].location.longitude).withColumn('sensor_id', dfs[i].sensor.id).drop('location', 'sensor')\n",
    "        # Explode sensordatavalues using pyspark.sql.functions.explode\n",
    "        df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # Remove rows that aren't P1 or P2:\n",
    "        df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "        # Remove rows that have negative values:\n",
    "        df_ = df_[df_.sensordatavalues.value >= 0]\n",
    "        # Regroup sensordatavalues by record id:\n",
    "        df_ = df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "        # Remove the old sensordatavalues column still containing values different from P1 and P2:\n",
    "        dfs[i] = dfs[i].drop('sensordatavalues')\n",
    "        # Link the new sensordatavalues column to the old dataframe, on id:\n",
    "        dfs[i] = dfs[i].join(df_, on='id', how='inner')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task1(dfs, air):\n",
    "    for i in range(len(dfs)):\n",
    "        # For task 1, only the country and sensordatavalues columns are needed:\n",
    "        df_ = dfs[i].select('country', 'sensordatavalues')\n",
    "        # Start by sorting by country:\n",
    "        df_ = df_.sort('country')\n",
    "        # explode sensordatavalues:\n",
    "        df_ = df_.withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # group by country:\n",
    "        df_ = df_.groupby('country').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "        # Create a RDD collection of tuples (country, (P1, P2)), so that each rdd element is a combo of a country and either its P1 or P2 values. Then convert RDDs to DataFrames and join them to the original dataframe:\n",
    "        df_ = df_.join(df_.rdd.map(lambda x: (x[0], [float(y['value']) for y in x[1] if y['value_type'] == 'P1'])).toDF(['country', 'P1']), on='country', how='inner')\n",
    "        df_ = df_.join(df_.rdd.map(lambda x: (x[0], [float(y['value']) for y in x[1] if y['value_type'] == 'P2'])).toDF(['country', 'P2']), on='country', how='inner')\n",
    "        # Create a RDD collection to calculate the average P1 and P2 values for each country, and convert RDDs to DataFrames and join them to the original dataframe:\n",
    "        df_ = df_.join(df_.rdd.map(lambda x: (x[0], float(np.round(np.mean(x[2]), 2)))).toDF(['country', 'avgP1']), on='country', how='inner')\n",
    "        df_ = df_.join(df_.rdd.map(lambda x: (x[0], float(np.round(np.mean(x[3]), 2)))).toDF(['country', 'avgP2']), on='country', how='inner')\n",
    "        # Associate P1 and P2 avg to their respective AQI:\n",
    "        df_ = df_.join(df_.rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[4]) <= air[y][2][1]][0])).toDF(['country', 'P1_AQI']), on='country', how='inner')\n",
    "        df_ = df_.join(df_.rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[5]) <= air[y][2][1]][0])).toDF(['country', 'P2_AQI']), on='country', how='inner')\n",
    "        # Calculate the max AQI for each country:\n",
    "        dfs[i] = df_.withColumn('maxAQI', when(df_.P1_AQI > df_.P2_AQI, df_.P1_AQI).otherwise(df_.P2_AQI))\n",
    "    # Choosing the last two dataframes, identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each.\n",
    "    df1 = dfs[len(dfs)-2].select('country', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "    df2 = dfs[len(dfs)-1].select('country', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "    df_diff = df1.join(df2, on='country', how='inner')\n",
    "    df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('country', 'diffAQI')\n",
    "    df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Task 1:\n",
    "air = air_quality_map()\n",
    "dfs = load_data(False)\n",
    "dfs = preprocessing(dfs)\n",
    "df_diff = task1(dfs, air)\n",
    "df_diff.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 2: Using the geo-coordinates from the sensor data, group the data into smaller regions using an appropriate clustering algorithm. Then determine the top 50 regions in terms of air quality improvement over the previous 24 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2-1: Pre-filter the pre-processed data by creating clusters and grouping data by cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_data_by_cluster(dfs):\n",
    "    # Create a RDD with the sensor id, and a tuple of the latitude and longitude, using the oldest dataframe:\n",
    "    rdd = dfs[0].rdd.map(lambda x: (x[4], (float(x[2]), float(x[3]))))\n",
    "    # Create a KMeans model with 200 clusters using latitude and longitude values from the first dataframe:\n",
    "    model = KMeans.train(rdd.map(lambda x: x[1]), 200, seed=23)\n",
    "    # (This model will be used to predict the cluster for each sensor id in the second dataframe.)\n",
    "\n",
    "    for i in range(len(dfs)):\n",
    "        # Create a RDD with the sensor id, and a tuple of the latitude and longitude using the current dataframe:\n",
    "        rdd = dfs[i].rdd.map(lambda x: (x[4], (float(x[2]), float(x[3]))))\n",
    "        # Create a RDD collection with both the sensor id and the corresponding predicted cluster:\n",
    "        rdd_clusters = rdd.map(lambda x: (x[0], model.predict(x[1])))\n",
    "        # Create a RDD containing the amount of sensors in each cluster:\n",
    "        rdd_sensorAmountByCluster = rdd_clusters.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "        # Store each cluster center in a dictionary (Each cluster center is a tuple of latitude and longitude), rounded to 2 decimals:\n",
    "        centers = {i: np.round(np.array(model.clusterCenters[i]), 2) for i in range(len(model.clusterCenters))}\n",
    "        # Add the cluster center to the RDD collection:\n",
    "        rdd_clusters = rdd_clusters.map(lambda x: (x[1], x[0], centers[x[1]].tolist()))\n",
    "        # Convert RDDs to DataFrames and join them to the original dataframe:\n",
    "        dfs[i] = dfs[i].join(rdd_clusters.toDF(['cluster_id', 'sensor_id', 'cluster_center']), on='sensor_id', how='inner')\n",
    "        dfs[i] = dfs[i].join(rdd_sensorAmountByCluster.toDF(['cluster_id', 'sensor_amount']), on='cluster_id', how='inner')\n",
    "        # Group by cluster, keeping the cluster center and the sensordatavalues:\n",
    "        dfs[i] = dfs[i].groupby('cluster_id').agg(collect_list('cluster_center')[0].alias('cluster_center'), collect_list('sensor_amount')[0].alias('sensor_amount'), collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "        # PROBLEM: sensordatavalues is a list of list of data values. To fix this, explode two times the sensordatavalues column:\n",
    "        dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # Then regroup by cluster, keeping the cluster center and the sensordatavalues for each cluster id:\n",
    "        dfs[i] = dfs[i].groupby('cluster_id').agg(collect_list('cluster_center')[0].alias('cluster_center'), collect_list('sensor_amount')[0].alias('sensor_amount'), collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "        # Once again, just like task 1, we compute the max AQI for each cluster:\n",
    "        # Create a RDD collection of tuples (country, (P1, P2)), so that each rdd element is a combo of a cluster and either its P1 or P2 values. Then convert RDDs to DataFrames and join them to the original dataframe:\n",
    "        dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [float(y['value']) for y in x[3] if y[2] == 'P1'])).toDF(['cluster_id', 'P1']), on='cluster_id', how='inner')\n",
    "        dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [float(y['value']) for y in x[3] if y[2] == 'P2'])).toDF(['cluster_id', 'P2']), on='cluster_id', how='inner')\n",
    "        # Create a RDD collection to calculate the average P1 and P2 values for each cluster, and convert RDDs to DataFrames and join them to the original dataframe:\n",
    "        dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], float(np.round(np.mean(x[4]), 2)))).toDF(['cluster_id', 'avgP1']), on='cluster_id', how='inner')\n",
    "        dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], float(np.round(np.mean(x[5]), 2)))).toDF(['cluster_id', 'avgP1']), on='cluster_id', how='inner')\n",
    "        # Associate P1 and P2 avg to their respective AQI:\n",
    "        dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[6]) <= air[y][2][1]][0])).toDF(['cluster_id', 'P1_AQI']), on='cluster_id', how='inner')\n",
    "        dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[7]) <= air[y][2][1]][0])).toDF(['cluster_id', 'P2_AQI']), on='cluster_id', how='inner')\n",
    "        # Calculate the max AQI for each cluster:\n",
    "        dfs[i] = dfs[i].withColumn('maxAQI', when(dfs[i].P1_AQI > dfs[i].P2_AQI, dfs[i].P1_AQI).otherwise(dfs[i].P2_AQI))\n",
    "        dfs[i] = dfs[i].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2-2: Select last two dataframes and compare their AQI to sort clusters by the AQI difference over the last 24 hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task2(dfs):\n",
    "    # Filter the dataframes by cluster:\n",
    "    dfs = filter_data_by_cluster(dfs)\n",
    "    # Select the last two dataframes, to compare the evolution of the air quality between the last 24 hours:\n",
    "    df1 = dfs[len(dfs)-2].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "    df2 = dfs[len(dfs)-1].select('cluster_id', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "    # Join both dataframes on cluster_id:\n",
    "    df_diff = df1.join(df2, on='cluster_id', how='inner')\n",
    "    # Create a column named diffAQI, whose value is the relative difference between today's maxAQI, and yesterday maxAQI:\n",
    "    df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('cluster_id', 'cluster_center', 'sensor_amount', 'diffAQI')\n",
    "    # Sort the dataframe by diffAQI, starting with the lowest diffAQIs:\n",
    "    df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "air = air_quality_map()\n",
    "dfs = load_data(False)\n",
    "dfs = preprocessing(dfs)\n",
    "df_diff = task2(dfs)\n",
    "df_diff.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Calculate the longest streaks of good air quality (ie low index values) and display as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task3(dfs):\n",
    "    # Filter the dataframes by cluster:\n",
    "    dfs = filter_data_by_cluster(dfs)\n",
    "    # Create a RDD with the cluster id, and a list containing 0s or 1s if the maxAQI is respectively lower or higher than 3:\n",
    "    rdd = dfs[0].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1 ]))\n",
    "    # Create a RDD with the cluster id, the list of 0s/1s, the current streak of repetitive 0s and the max streak:\n",
    "    rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][0] == 0 else 0), (1 if x[1][0] == 0 else 0)))\n",
    "    # Convert the RDD to a dataframe:\n",
    "    df = rdd_streaks.toDF(['cluster_id', 'streaks', 'current_streak', 'max_streak'])\n",
    "    # For all the following days (each dataframes following the first stored one)\n",
    "    for i in range(1, len(dfs)):\n",
    "        # Create a RDD with the cluster id, and a list containing 0s or 1s if the maxAQI is respectively lower or higher than 3:\n",
    "        rdd = dfs[i].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1 ]))\n",
    "        # Create a RDD with the cluster id, the list of 0s/1s and the current streak:\n",
    "        rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][len(x[1])-1] == 0 else 0)))\n",
    "        # Convert the RDD containing streak information to a dataframe, and join it to the previous dataframe:\n",
    "        df = df.join(rdd_streaks.toDF(['cluster_id', 'streak', 'previous_streak']), on='cluster_id', how='inner')\n",
    "        # Concatenate the 0s/1s values list with the current df 0s/1s value into a single list, and drop the colomn with only one value:\n",
    "        df = df.withColumn('streaks', concat('streaks', 'streak'))\n",
    "        df = df.drop('streak')\n",
    "        # Update the current_streak column using the previous_streak value:\n",
    "        df = df.withColumn('current_streak', when(df.previous_streak == 1, df.current_streak + 1).otherwise(0))\n",
    "        # Update the max_streak value using the previous_streak and the max_streak:\n",
    "        df = df.withColumn('max_streak', when(df.current_streak > df.max_streak, df.current_streak).otherwise(df.max_streak))\n",
    "        # Drop the current streak value:\n",
    "        df = df.drop('previous_streak')\n",
    "    # Show the current state of streaks for each cluster id (for verification):\n",
    "    df = df.sort('cluster_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "air = air_quality_map()\n",
    "dfs = load_data(True)\n",
    "dfs = preprocessing(dfs)\n",
    "df = task3(dfs)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Two histogram methods:\n",
    "Method 1: A single histogram showing longest (maximum or average) streaks across all regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def method_one_histogram(df):\n",
    "    # Group by max_streak, show the cluster_id and the amount of clusters with that max_streak:\n",
    "    df = df.groupBy('max_streak').agg(count('cluster_id').alias('cluster_amount'), collect_list('cluster_id').alias('cluster_ids'))\n",
    "    df = df.sort('max_streak', ascending=False)\n",
    "    print('The clusters ids with the longest streaks of good air quality (ie low index values), as well as the amount of clusters with that streak:')\n",
    "    df.show(df.count())\n",
    "    # Create histogram of the streaks:\n",
    "    df = df.withColumn('max_streak', df.max_streak.cast('int'))\n",
    "    df = df.withColumn('cluster_amount', df.cluster_amount.cast('int'))\n",
    "    pdf = df.sort('max_streak', ascending=True).toPandas()\n",
    "    pdf.plot.bar(x='max_streak', y='cluster_amount', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df1 = method_one_histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Method 2: A histogram for each region/cluster, showing the distribution of continuous good AQI streaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def method_two_histogram(df):\n",
    "    # Calculate successive 0s for each cluster:\n",
    "    pdf = df.toPandas()\n",
    "    max_streak = len(dfs)\n",
    "    # Create a list of lists, where each list contains the successive 0s for each cluster:\n",
    "    pdf['successive_0s'] = pdf['streaks'].apply(lambda x: [len(list(g)) for k, g in groupby(x) if k == 0])\n",
    "    # Count the amount of clusters with a certain amount of successive 0s:\n",
    "    pdf['successive_0s_hist'] = pdf['successive_0s'].apply(lambda x: [x.count(i) for i in range(1, max_streak+1)])\n",
    "    # Deduce the amount of time the cluster didn't came back to a good air quality:\n",
    "    pdf['successive_0s_hist'] = pdf.apply(lambda x: [max_streak - np.sum(x['successive_0s'])] + x['successive_0s_hist'], axis=1)\n",
    "    # Remove useless columns:\n",
    "    pdf = pdf.drop('streaks', axis=1).drop('current_streak', axis=1).drop('max_streak', axis=1)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df2 = method_two_histogram(df)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Create a histogram of the successive 0s with a slider to select the cluster id using matplotlib:\n",
    "# def f(cluster_id):\n",
    "#     # Fix it so that you can plot using the cluster_id from a spark dataframe:\n",
    "#     plt.bar(range(0, len(dfs)+1), df2.iloc[cluster_id]['successive_0s_hist'])\n",
    "#     plt.show()\n",
    "# interact(f, cluster_id=widgets.IntSlider(min=0, max=len(df2)-1, step=1, value=0))\n",
    "plt.bar(range(0, len(dfs)+1), df2.iloc[3]['successive_0s_hist'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Folium map with three sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
