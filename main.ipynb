{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "\n",
    "url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "# with urllib.request.urlopen(url) as url:\n",
    "#     data = json.load(url)\n",
    "# with urllib.request.urlopen(url24h) as url:\n",
    "#     data24h = json.load(url)\n",
    "# with open('data/data.json', 'w') as outfile:\n",
    "#     json.dump(data, outfile)\n",
    "# with open('data/data24h.json', 'w') as outfile:\n",
    "#     json.dump(data24h, outfile)\n",
    "\n",
    "# Save csv to array\n",
    "air = np.genfromtxt('data/air.csv', delimiter=',', dtype=None, encoding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "spark.sparkContext.addFile(url24h)\n",
    "filename = SparkFiles.get(url24h.split('/')[-1])\n",
    "df24h = spark.read.json(filename)\n",
    "df24h.createOrReplaceTempView(\"sensors24h\")\n",
    "df24h.printSchema()\n",
    "\n",
    "spark.sparkContext.addFile(url5min)\n",
    "filename = SparkFiles.get(url5min.split('/')[-1])\n",
    "df5min = spark.read.json(filename)\n",
    "df5min.createOrReplaceTempView(\"sensors5min\")\n",
    "df5min.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove columns that do not contains neither PM10 nor PM2.5 :\n",
    "df = df.\\\n",
    "    where(array_contains(col('sensordatavalues.value_type'), 'P1')\n",
    "          | array_contains(col('sensordatavalues.value_type'),'P2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove everything but P1 and P2 in df, by creating a new df using pandas and exploding the sensordatavalues column:\n",
    "df.select('id', 'sensordatavalues.value_type', 'sensordatavalues.value').show(10, False)    # BEFORE\n",
    "\n",
    "pdf = df.toPandas()\n",
    "pdf = pdf.explode('sensordatavalues')\n",
    "pdf = pdf[pdf['sensordatavalues'].apply(lambda x: x['value_type'] in ['P1', 'P2'])]\n",
    "pdf = pdf.groupby('id').agg({'sensordatavalues': lambda x: list(x)})\n",
    "pdf['id'] = pdf.index\n",
    "\n",
    "df2 = spark.createDataFrame(pdf)\n",
    "\n",
    "df = df.drop('sensordatavalues')\n",
    "df = df.join(df2, on='id')\n",
    "\n",
    "df.select('id', 'sensordatavalues.value_type', 'sensordatavalues.value').show(10, False)    # AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
