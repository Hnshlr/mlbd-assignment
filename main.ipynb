{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MSc in CSTE, CIDA option Machine learning & Big Data Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Analysis of data from an environmental sensor network using Hadoop/Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start a timer:\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import *\n",
    "import urllib.request, json, datetime\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from itertools import groupby\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import branca\n",
    "from folium.plugins import HeatMap\n",
    "import folium\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SETTINGS=\n",
    "MAX_MEMORY= \"8g\"\n",
    "\n",
    "# Spark session builder:\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\", MAX_MEMORY).config(\"spark.driver.memory\", MAX_MEMORY).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"OFF\")\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Locally save instances of the data:\n",
    "def save_data(wait):\n",
    "    # URLs of the data:\n",
    "    url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "    url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "    # If wait=True, sleep until specific time (e.g. 5pm) before running the next line\n",
    "    if wait:\n",
    "        today = datetime.datetime.now()\n",
    "        exactImportTime = datetime.datetime(today.year, today.month, today.day, 17, 0, 0)\n",
    "        awaitingTime = exactImportTime - today\n",
    "        time.sleep(awaitingTime.total_seconds())\n",
    "\n",
    "    # Download the data, saved as json files:\n",
    "    today = datetime.datetime.now()\n",
    "    with urllib.request.urlopen(url5min) as url:\n",
    "        data5min = json.load(url)\n",
    "    with open('output/5min/data5min_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "        json.dump(data5min, outfile)\n",
    "    with urllib.request.urlopen(url24h) as url:\n",
    "        data24h = json.load(url)\n",
    "    with open('output/24h/data24h_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "        json.dump(data24h, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from local files and load them into Spark DataFrames:\n",
    "def load_data(all):\n",
    "    path = 'output/24h/'\n",
    "    file1 = 'data24h_2022-11-3_17h00.json'\n",
    "    file2 = 'data24h_2022-11-4_17h00.json'\n",
    "    file3 = 'data24h_2022-11-5_17h00.json'\n",
    "    file4 = 'data24h_2022-11-6_17h00.json'\n",
    "    file5 = 'data24h_2022-11-7_17h00.json'\n",
    "    file6 = 'data24h_2022-11-8_17h00.json'\n",
    "    file7 = 'data24h_2022-11-9_17h00.json'\n",
    "    file8 = 'data24h_2022-11-10_17h00.json'\n",
    "    file9 = 'data24h_2022-11-11_17h00.json'\n",
    "    file10 = 'data24h_2022-11-12_17h00.json'\n",
    "    file11 = 'data24h_2022-11-13_17h00.json'\n",
    "    file12 = 'data24h_2022-11-14_17h00.json'\n",
    "    file13 = 'data24h_2022-11-15_17h00.json'\n",
    "    file14 = 'data24h_2022-11-16_17h00.json'\n",
    "    file15 = 'data24h_2022-11-17_17h00.json'\n",
    "\n",
    "    files = [file1, file2, file3, file4, file5, file6, file7, file8, file9, file10, file11, file12, file13, file14, file15]\n",
    "    dfs = []\n",
    "\n",
    "    if all:\n",
    "        for file in files:\n",
    "            spark.sparkContext.addFile(path + file)\n",
    "            filename = SparkFiles.get(file)\n",
    "            df = spark.read.json(filename)\n",
    "            dfs.append(df)\n",
    "    else:\n",
    "        for file in files[len(files)-2:]:\n",
    "            spark.sparkContext.addFile(path + file)\n",
    "            filename = SparkFiles.get(file)\n",
    "            df = spark.read.json(filename)\n",
    "            dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark implementation & tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 0: Data pre-processing, filtering and cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing (P1 and P2 filtering) (Common to all tasks, the returned dataframe will be used for all three tasks):\n",
    "def preprocessing(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        # Remove columns that are not needed for all three tasks:\n",
    "        dfs[i] = dfs[i].drop('sampling_rate', 'timestamp').withColumn('country', dfs[i].location.country).withColumn('latitude', dfs[i].location.latitude.cast('float')).withColumn('longitude', dfs[i].location.longitude.cast('float')).withColumn('sensor_id', dfs[i].sensor.id).drop('location', 'sensor')\n",
    "        # Explode sensordatavalues using pyspark.sql.functions.explode\n",
    "        df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # Remove rows that aren't P1 or P2:\n",
    "        df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "        # Remove rows that have negative values:\n",
    "        df_ = df_[df_.sensordatavalues.value >= 0]\n",
    "        # Regroup sensordatavalues by record id:\n",
    "        df_ = df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "        # Remove the old sensordatavalues column still containing values different from P1 and P2:\n",
    "        dfs[i] = dfs[i].drop('sensordatavalues')\n",
    "        # Link the new sensordatavalues column to the old dataframe, on id:\n",
    "        dfs[i] = dfs[i].join(df_, on='id', how='inner')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_by_country(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        # Explode sensordatavalues column:\n",
    "        dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # Create a column only for P1 and P2 values:\n",
    "        dfs[i] = dfs[i].withColumn('P1', when(dfs[i].sensordatavalues.value_type == 'P1', dfs[i].sensordatavalues.value).otherwise(None))\n",
    "        dfs[i] = dfs[i].withColumn('P2', when(dfs[i].sensordatavalues.value_type == 'P2', dfs[i].sensordatavalues.value).otherwise(None))\n",
    "        # Group by country and calculate the mean of P1 and P2 for each cluster, and keep the sensor_amount:\n",
    "        dfs[i] = dfs[i].groupBy('country').agg(mean('P1').alias('avgP1'), mean('P2').alias('avgP2'))\n",
    "        # Map the average P1 and P2 values to the AQI scale using when statements:\n",
    "        dfs[i] = dfs[i].withColumn('avgP1_AQI', when(dfs[i].avgP1 < 17, 1).otherwise(\n",
    "            when(dfs[i].avgP1 < 34, 2).otherwise(\n",
    "                when(dfs[i].avgP1 < 51, 3).otherwise(\n",
    "                    when(dfs[i].avgP1 < 59, 4).otherwise(\n",
    "                        when(dfs[i].avgP1 < 67, 5).otherwise(\n",
    "                            when(dfs[i].avgP1 < 76, 6).otherwise(\n",
    "                                when(dfs[i].avgP1 < 84, 7).otherwise(\n",
    "                                    when(dfs[i].avgP1 < 92, 8).otherwise(\n",
    "                                        when(dfs[i].avgP1 < 101, 9).otherwise(10))))))))))\n",
    "        dfs[i] = dfs[i].withColumn('avgP2_AQI', when(dfs[i].avgP2 < 12, 1).otherwise(\n",
    "            when(dfs[i].avgP2 < 24, 2).otherwise(\n",
    "                when(dfs[i].avgP2 < 36, 3).otherwise(\n",
    "                    when(dfs[i].avgP2 < 42, 4).otherwise(\n",
    "                        when(dfs[i].avgP2 < 48, 5).otherwise(\n",
    "                            when(dfs[i].avgP2 < 54, 6).otherwise(\n",
    "                                when(dfs[i].avgP2 < 59, 7).otherwise(\n",
    "                                    when(dfs[i].avgP2 < 65, 8).otherwise(\n",
    "                                        when(dfs[i].avgP2 < 71, 9).otherwise(10))))))))))\n",
    "        # Calculate the max AQI value for each cluster:\n",
    "        dfs[i] = dfs[i].withColumn('maxAQI', when(dfs[i].avgP1_AQI > dfs[i].avgP2_AQI, dfs[i].avgP1_AQI).otherwise(dfs[i].avgP2_AQI))\n",
    "        dfs[i] = dfs[i].select('country', 'maxAQI')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task1(dfs):\n",
    "    # Choosing the last two dataframes, identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each.\n",
    "    df1 = dfs[len(dfs)-2].select('country', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "    df2 = dfs[len(dfs)-1].select('country', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "    df_diff = df1.join(df2, on='country', how='inner')\n",
    "    df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('country', 'diffAQI')\n",
    "    df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 2: Using the geo-coordinates from the sensor data, group the data into smaller regions using an appropriate clustering algorithm. Then determine the top 50 regions in terms of air quality improvement over the previous 24 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2-1: Pre-filter the pre-processed data by creating clusters and grouping data by cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_by_cluster(dfs, amount):\n",
    "    # Create a KMeans model fitting on the first dataframe, using the latitude and logitude columns, combined together in a features column used to predict the future coordinates:\n",
    "    model = KMeans(k=amount, seed=23, featuresCol='features', predictionCol='cluster_id').fit(VectorAssembler(inputCols=['latitude', 'longitude'], outputCol='features').transform(dfs[0]))\n",
    "    # (This model will be used to predict the cluster for each sensor id in the second dataframe.)\n",
    "    for i in range(len(dfs)):\n",
    "        # Transform the dataframe:\n",
    "        dfs[i] = VectorAssembler(inputCols=['latitude', 'longitude'], outputCol='features').transform(dfs[i])\n",
    "        # Predict the cluster for each sensor id:\n",
    "        dfs[i] = model.transform(dfs[i])\n",
    "        # Explode sensordatavalues column:\n",
    "        dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # Create a column only for P1 and P2 values:\n",
    "        dfs[i] = dfs[i].withColumn('P1', when(dfs[i].sensordatavalues.value_type == 'P1', dfs[i].sensordatavalues.value).otherwise(None))\n",
    "        dfs[i] = dfs[i].withColumn('P2', when(dfs[i].sensordatavalues.value_type == 'P2', dfs[i].sensordatavalues.value).otherwise(None))\n",
    "        # Create a column for the coordinates, an array of latitude and longitude:\n",
    "        dfs[i] = dfs[i].withColumn('coordinates', array('latitude', 'longitude'))\n",
    "        # Group by cluster_id and calculate the mean of P1 and P2 for each cluster, and keep the sensor_amount and a list of sensor_ids as well as the coordinates:\n",
    "        dfs[i] = dfs[i].groupBy('cluster_id').agg(mean('P1').alias('avgP1'), mean('P2').alias('avgP2'), count('sensor_id').alias('sensor_amount'), collect_list('sensor_id').alias('sensor_ids'), collect_list('coordinates').alias('coordinates'))\n",
    "\n",
    "        # Store each cluster's center in the dataframe:\n",
    "        cluster_centers = []\n",
    "        for k in range(len(model.clusterCenters())):\n",
    "            cluster_centers.append([float(np.round(model.clusterCenters()[k][0], 2)), float(np.round(model.clusterCenters()[k][1], 2))])\n",
    "        cluster_ids = [i for i in range(len(cluster_centers))]\n",
    "        # Create a dataframe containing the cluster_id and the cluster_center:\n",
    "        df_cluster_centers = spark.createDataFrame(list(zip(cluster_ids, cluster_centers)), ['cluster_id', 'cluster_center'])\n",
    "        # Join the two dataframes:\n",
    "        dfs[i] = dfs[i].join(df_cluster_centers, on='cluster_id', how='inner')\n",
    "\n",
    "        # Map the average P1 and P2 values to the AQI scale using when statements:\n",
    "        dfs[i] = dfs[i].withColumn('avgP1_AQI', when(dfs[i].avgP1 < 17, 1).otherwise(\n",
    "            when(dfs[i].avgP1 < 34, 2).otherwise(\n",
    "                when(dfs[i].avgP1 < 51, 3).otherwise(\n",
    "                    when(dfs[i].avgP1 < 59, 4).otherwise(\n",
    "                        when(dfs[i].avgP1 < 67, 5).otherwise(\n",
    "                            when(dfs[i].avgP1 < 76, 6).otherwise(\n",
    "                                when(dfs[i].avgP1 < 84, 7).otherwise(\n",
    "                                    when(dfs[i].avgP1 < 92, 8).otherwise(\n",
    "                                        when(dfs[i].avgP1 < 101, 9).otherwise(10))))))))))\n",
    "        dfs[i] = dfs[i].withColumn('avgP2_AQI', when(dfs[i].avgP2 < 12, 1).otherwise(\n",
    "            when(dfs[i].avgP2 < 24, 2).otherwise(\n",
    "                when(dfs[i].avgP2 < 36, 3).otherwise(\n",
    "                    when(dfs[i].avgP2 < 42, 4).otherwise(\n",
    "                        when(dfs[i].avgP2 < 48, 5).otherwise(\n",
    "                            when(dfs[i].avgP2 < 54, 6).otherwise(\n",
    "                                when(dfs[i].avgP2 < 59, 7).otherwise(\n",
    "                                    when(dfs[i].avgP2 < 65, 8).otherwise(\n",
    "                                        when(dfs[i].avgP2 < 71, 9).otherwise(10))))))))))\n",
    "        # Calculate the max AQI value for each cluster:\n",
    "        dfs[i] = dfs[i].withColumn('maxAQI', when(dfs[i].avgP1_AQI > dfs[i].avgP2_AQI, dfs[i].avgP1_AQI).otherwise(dfs[i].avgP2_AQI))\n",
    "        dfs[i] = dfs[i].select('cluster_id', 'cluster_center', 'sensor_amount', 'sensor_ids', 'coordinates', 'maxAQI').sort('cluster_id')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2-2: Select last two dataframes and compare their AQI to sort clusters by the AQI difference over the last 24 hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task2(dfs):\n",
    "    # Select the last two dataframes, to compare the evolution of the air quality between the last 24 hours:\n",
    "    df1 = dfs[len(dfs)-2].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "    df2 = dfs[len(dfs)-1].select('cluster_id', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "    # Join both dataframes on cluster_id:\n",
    "    df_diff = df1.join(df2, on='cluster_id', how='inner')\n",
    "    # Create a column named diffAQI, whose value is the relative difference between today's maxAQI, and yesterday maxAQI:\n",
    "    df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('cluster_id', 'cluster_center', 'sensor_amount', 'diffAQI')\n",
    "    # Sort the dataframe by diffAQI, starting with the lowest diffAQIs:\n",
    "    df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Calculate the longest streaks of good air quality (ie low index values) and display as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task3(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i] = dfs[i].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI').sort('cluster_id')\n",
    "    # Create a RDD with the cluster id, and a list containing 0s or 1s if the maxAQI is respectively lower or higher than 3:\n",
    "    rdd = dfs[0].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1]))\n",
    "    # Create a RDD with the cluster id, the list of 0s/1s, the current streak of repetitive 0s and the max streak:\n",
    "    rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][0] == 0 else 0), (1 if x[1][0] == 0 else 0)))\n",
    "    # Convert the RDD to a dataframe:\n",
    "    df = rdd_streaks.toDF(['cluster_id', 'streaks', 'current_streak', 'max_streak'])\n",
    "    # For all the following days (each dataframes following the first stored one)\n",
    "    for i in range(1, len(dfs)):\n",
    "        # Create a RDD with the cluster id, and a list containing 0s or 1s if the maxAQI is respectively lower or higher than 3:\n",
    "        rdd = dfs[i].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1]))\n",
    "        # Create a RDD with the cluster id, the list of 0s/1s and the current streak:\n",
    "        rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][len(x[1])-1] == 0 else 0)))\n",
    "        # Convert the RDD containing streak information to a dataframe, and join it to the previous dataframe:\n",
    "        df = df.join(rdd_streaks.toDF(['cluster_id', 'streak', 'previous_streak']), on='cluster_id', how='inner')\n",
    "        # Concatenate the 0s/1s values list with the current df 0s/1s value into a single list, and drop the colomn with only one value:\n",
    "        df = df.withColumn('streaks', concat('streaks', 'streak'))\n",
    "        df = df.drop('streak')\n",
    "        # Update the current_streak column using the previous_streak value:\n",
    "        df = df.withColumn('current_streak', when(df.previous_streak == 1, df.current_streak + 1).otherwise(0))\n",
    "        # Update the max_streak value using the previous_streak and the max_streak:\n",
    "        df = df.withColumn('max_streak', when(df.current_streak > df.max_streak, df.current_streak).otherwise(df.max_streak))\n",
    "        # Drop the current streak value:\n",
    "        df = df.drop('previous_streak')\n",
    "    # Show the current state of streaks for each cluster id (for verification):\n",
    "    df = df.sort('cluster_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Two histogram methods:\n",
    "Method 1: A single histogram showing longest (maximum or average) streaks across all regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def method_one_histogram(df):\n",
    "    # Group by max_streak, show clusters ids with such max streak, as well as their amount:\n",
    "    df = df.groupBy('max_streak').agg(count('cluster_id').alias('cluster_amount'), collect_list('cluster_id').alias('cluster_ids'))\n",
    "    # Cast the max_streak and cluster_amount columns to int:\n",
    "    df = df.withColumn('max_streak', df.max_streak.cast('int')).withColumn('cluster_amount', df.cluster_amount.cast('int'))\n",
    "    # Sort the dataframe by max_streak:\n",
    "    df = df.sort('max_streak', ascending=False)\n",
    "    pdf = df.toPandas()\n",
    "    pdf.plot.bar(x='max_streak', y='cluster_amount', rot=0)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Method 2: A histogram for each region/cluster, showing the distribution of continuous good AQI streaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def method_two_histogram(df):\n",
    "    # Calculate successive 0s for each cluster:\n",
    "    pdf = df.toPandas()\n",
    "    max_streak = len(dataframes)\n",
    "    # Create a list of lists, where each list contains the successive 0s for each cluster:\n",
    "    pdf['successive_0s'] = pdf['streaks'].apply(lambda x: [len(list(g)) for k, g in groupby(x) if k == 0])\n",
    "    # Count the amount of clusters with a certain amount of successive 0s:\n",
    "    pdf['successive_0s_hist'] = pdf['successive_0s'].apply(lambda x: [x.count(i) for i in range(1, max_streak+1)])\n",
    "    # Deduce the amount of time the cluster didn't came back to a good air quality:\n",
    "    pdf['successive_0s_hist'] = pdf.apply(lambda x: [max_streak - np.sum(x['successive_0s'])] + x['successive_0s_hist'], axis=1)\n",
    "    # Remove useless columns:\n",
    "    pdf = pdf.drop('streaks', axis=1).drop('current_streak', axis=1).drop('max_streak', axis=1)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PROGRAM EXECUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load and pre-process the data:\n",
    "dataframes = preprocessing(load_data(True))\n",
    "\n",
    "# Task 1:\n",
    "df_tsk1 = task1(filter_by_country(dataframes[len(dataframes)-2:]))\n",
    "\n",
    "# Task 2:\n",
    "df_tsk2 = task2(filter_by_cluster(dataframes[len(dataframes)-2:], 200))\n",
    "\n",
    "# Task 3:\n",
    "dfs_fbcr = filter_by_cluster(dataframes[:], 50)\n",
    "df_tsk3 = task3(dfs_fbcr[:])\n",
    "\n",
    "# df1 = method_one_histogram(df_tsk3)           # Currently not using it directly, as it'll be used on every cluster on the Folium map.\n",
    "df2 = method_two_histogram(df_tsk3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Gather back the last day of data, filtered by clusters, in order to get the coordinates of the clusters:\n",
    "df = dfs_fbcr[-1]\n",
    "# Split cluster center coordinates into two columns, and drop the old cluster_center column:\n",
    "df = df.withColumn('center_latitude', df['cluster_center'][0]).withColumn('center_longitude', df['cluster_center'][1]).drop('cluster_center')\n",
    "# Convert the dataframe to a pandas dataframe:\n",
    "pdf = df.toPandas()\n",
    "# Gather back the pdf containing the successive 0s for each cluster, used for the second method histogram:\n",
    "pdf2 = df2.drop('successive_0s', axis=1)\n",
    "# Inner join the two dataframes:\n",
    "pdf = pdf.merge(pdf2, on='cluster_id', how='inner')\n",
    "\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Folium World Map:\n",
    "map = folium.Map(location=[30,0], zoom_start=3.2, tiles='OpenStreetMap', width='100%', height='100%')\n",
    "\n",
    "# Default markers:\n",
    "markers = folium.FeatureGroup(name='markers')\n",
    "for i in range(0, len(pdf)):\n",
    "    chart = alt.Chart(pd.DataFrame({'x': range(0, len(dataframes)+1), 'y': pdf.iloc[i]['successive_0s_hist']})).mark_bar(size=20).encode(\n",
    "        x='x',\n",
    "        y='y'\n",
    "    )\n",
    "    chart_json = json.loads(chart.to_json())\n",
    "    folium.Marker(\n",
    "        location=[pdf.iloc[i]['center_latitude'], pdf.iloc[i]['center_longitude']],\n",
    "        popup=folium.Popup(max_width=450).add_child(\n",
    "            folium.VegaLite(chart_json, width=450, height=250))\n",
    "    ).add_to(markers)\n",
    "markers.add_to(map)\n",
    "\n",
    "# AQI heatmap:\n",
    "heatmapgroup = folium.FeatureGroup(name='heatmap')\n",
    "heatmap = HeatMap(list(zip(pdf['center_latitude'], pdf['center_longitude'], pdf['maxAQI'])),\n",
    "                  min_opacity=0.2,\n",
    "                  radius=17, blur=15,\n",
    "                  max_zoom=1,\n",
    "                  gradient={0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'})\n",
    "heatmap.add_to(heatmapgroup)\n",
    "heatmapgroup.add_to(map)\n",
    "\n",
    "# Branca colormap:\n",
    "colormap = branca.colormap.LinearColormap(\n",
    "    colors=['blue', 'lime', 'orange', 'red'],\n",
    "    vmin=1,\n",
    "    vmax=10\n",
    ").to_step(index=[i for i in range(1,11)])\n",
    "colormap.caption = 'Air quality index'\n",
    "colormap.add_to(map)\n",
    "\n",
    "# Add a layer control to the map:\n",
    "folium.LayerControl().add_to(map)\n",
    "\n",
    "# Show the map:\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Stop the timer and the spark session:\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
