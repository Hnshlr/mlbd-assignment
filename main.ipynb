{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MSc in CSTE, CIDA option Machine learning & Big Data Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Analysis of data from an environmental sensor network using Hadoop/Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import *\n",
    "import urllib.request, json, datetime, time\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Allocate more memory to the driver:\n",
    "MAX_MEMORY= \"8g\"\n",
    "# Spark session builder:\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\", MAX_MEMORY).config(\"spark.driver.memory\", MAX_MEMORY).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"OFF\")\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Locally save instances of the data:\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "# sleep until specific time (e.g. 5pm) before running the next line\n",
    "today = datetime.datetime.now()\n",
    "exactImportTime = datetime.datetime(today.year, today.month, today.day, 17, 0, 0)\n",
    "awaitingTime = exactImportTime - today\n",
    "time.sleep(awaitingTime.total_seconds())\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "with urllib.request.urlopen(url5min) as url:\n",
    "    data5min = json.load(url)\n",
    "with open('output/5min/data5min_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "    json.dump(data5min, outfile)\n",
    "with urllib.request.urlopen(url24h) as url:\n",
    "    data24h = json.load(url)\n",
    "with open('output/24h/data24h_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "    json.dump(data24h, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from local files and load them into Spark DataFrames:\n",
    "path = 'output/24h/'\n",
    "file1 = 'data24h_2022-11-3_17h00.json'\n",
    "file2 = 'data24h_2022-11-4_17h00.json'\n",
    "file3 = 'data24h_2022-11-5_17h00.json'\n",
    "file4 = 'data24h_2022-11-6_17h00.json'\n",
    "file5 = 'data24h_2022-11-7_17h00.json'\n",
    "file6 = 'data24h_2022-11-8_17h00.json'\n",
    "\n",
    "files = [file1, file2, file3, file4, file5, file6]\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    spark.sparkContext.addFile(path + file)\n",
    "    filename = SparkFiles.get(file)\n",
    "    df = spark.read.json(filename)\n",
    "    df.printSchema()\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# AQI Map:\n",
    "air = {}\n",
    "air[1] = [\"Low\", [0,16], [0,11]]\n",
    "air[2] = [\"Low\", [17,33], [12,23]]\n",
    "air[3] = [\"Low\", [34,50], [24,35]]\n",
    "air[4] = [\"Medium\", [51,58], [36,41]]\n",
    "air[5] = [\"Medium\", [59,66], [42,47]]\n",
    "air[6] = [\"Medium\", [67,75], [48,53]]\n",
    "air[7] = [\"High\", [76,83], [54,58]]\n",
    "air[8] = [\"High\", [84,91], [59,64]]\n",
    "air[9] = [\"High\", [92,100], [65,70]]\n",
    "air[10] = [\"Very High\", [101,10000000], [71,10000000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark implementation & tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing (P1 and P2 filtering):\n",
    "for i in range(len(dfs)):\n",
    "    print(\"Raw dataset count: \", dfs[i].count())\n",
    "    # Explode sensordatavalues using pyspark.sql.functions.explode\n",
    "    df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    print(\"Dataset count after exploding sensordatavalues: \", df_.count())\n",
    "    # Remove rows that aren't P1 or P2:\n",
    "    df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "    print(\"Dataset count after removing rows that aren't P1 or P2: \", df_.count())\n",
    "    # Regroup sensordatavalues by record id:\n",
    "    df_ = df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Remove the old sensordatavalues column still containing values different from P1 and P2:\n",
    "    dfs[i] = dfs[i].drop('sensordatavalues')\n",
    "    # Link the new sensordatavalues column to the old dataframe, on id:\n",
    "    dfs[i] = dfs[i].join(df_, on='id', how='inner')\n",
    "    print(\"Dataset count after joining the new sensordatavalues column to the old dataframe: \", dfs[i].count())\n",
    "\n",
    "    dfs[i].show(5)\n",
    "    # dfs[i].select('id','location.country','location.id', 'sensordatavalues.value_type','sensordatavalues.value').sort('location.country', 'location.id').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    # replace location field with country field:\n",
    "    df_ = dfs[i].withColumn('location', dfs[i]['location.country'])\n",
    "    # sort by country:\n",
    "    df_ = df_.sort('location')\n",
    "    # explode sensordatavalues:\n",
    "    df_ = df_.withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    # group by country:\n",
    "    df_ = df_.groupby('location').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Create a RDD collection of tuples (country, (P1, P2)), so that each rdd element is a combo of a country and either its P1 or P2 values. Then convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [float(y['value']) for y in x[1] if y['value_type'] == 'P1'])).toDF(['location', 'P1']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [float(y['value']) for y in x[1] if y['value_type'] == 'P2'])).toDF(['location', 'P2']), on='location', how='inner')\n",
    "    # Create a RDD collection to calculate the average P1 and P2 values for each country, and convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], float(np.round(np.mean(x[2]), 2)))).toDF(['location', 'avgP1']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], float(np.round(np.mean(x[3]), 2)))).toDF(['location', 'avgP2']), on='location', how='inner')\n",
    "    # Associate P1 and P2 avg to their respective AQI:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[4]) <= air[y][2][1]][0])).toDF(['location', 'P1_AQI']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[5]) <= air[y][2][1]][0])).toDF(['location', 'P2_AQI']), on='location', how='inner')\n",
    "    # Calculate the max AQI for each country:\n",
    "    df_ = df_.withColumn('maxAQI', when(df_.P1_AQI > df_.P2_AQI, df_.P1_AQI).otherwise(df_.P2_AQI))\n",
    "\n",
    "    dfs[i] = df_\n",
    "    dfs[i].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each.\n",
    "df1 = dfs[0].select('location', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "df2 = dfs[1].select('location', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "df_diff = df1.join(df2, on='location', how='inner')\n",
    "df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('location', 'diffAQI')\n",
    "df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "df_diff.show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Using the geo-coordinates from the sensor data, group the data into smaller regions using an appropriate clustering algorithm. Then determine the top 50 regions in terms of air quality improvement over the previous 24 hours.\n",
    "\n",
    "Assume task 1 is not executed. Let's re-filter the dataframes for P1 and P2 values, and then group by clusters instead of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    # Same process as task 1, we filter the dataframe to keep only P1 and P2 values\n",
    "    df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "    dfs[i] = dfs[i].drop('sensordatavalues').join(df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues')), on='id', how='inner')\n",
    "    dfs[i] = dfs[i].select('sensor.id','location.latitude','location.longitude', 'location.altitude', 'sensordatavalues').sort('sensor.id')\n",
    "\n",
    "# TODO: Find optimal amount of clusters\n",
    "\n",
    "# Create a RDD with the sensor id, and a tuple of the latitude and longitude, using the oldest dataframe:\n",
    "rdd = dfs[0].rdd.map(lambda x: (x[0], (float(x[1]), float(x[2]))))\n",
    "# Create a KMeans model with 200 clusters using latitude and longitude values from the first dataframe:\n",
    "model = KMeans.train(rdd.map(lambda x: x[1]), 200, seed=23)\n",
    "# (This model will be used to predict the cluster for each sensor id in the second dataframe.)\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    # Create a RDD with the sensor id, and a tuple of the latitude and longitude using the current dataframe:\n",
    "    rdd = dfs[i].rdd.map(lambda x: (x[0], (float(x[1]), float(x[2]))))\n",
    "    # Create a RDD collection with both the sensor id and the corresponding predicted cluster:\n",
    "    rdd_clusters = rdd.map(lambda x: (x[0], model.predict(x[1])))\n",
    "    # Create a RDD containing the amount of sensors in each cluster:\n",
    "    rdd_sensorAmountByCluster = rdd_clusters.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "    # Store each cluster center in a dictionary (Each cluster center is a tuple of latitude and longitude), rounded to 2 decimals:\n",
    "    centers = {i: np.round(np.array(model.clusterCenters[i]), 2) for i in range(len(model.clusterCenters))}\n",
    "    # Add the cluster center to the RDD collection:\n",
    "    rdd_clusters = rdd_clusters.map(lambda x: (x[1], x[0], centers[x[1]].tolist()))\n",
    "    # Convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(rdd_clusters.toDF(['cluster_id', 'id', 'cluster_center']), on='id', how='inner')\n",
    "    dfs[i] = dfs[i].join(rdd_sensorAmountByCluster.toDF(['cluster_id', 'sensor_amount']), on='cluster_id', how='inner')\n",
    "    # Group by cluster, keeping the cluster center and the sensordatavalues:\n",
    "    dfs[i] = dfs[i].groupby('cluster_id').agg(collect_list('cluster_center')[0].alias('cluster_center'), collect_list('sensor_amount')[0].alias('sensor_amount'), collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # PROBLEM: sensordatavalues is a list of list of data values. To fix this, explode two times the sensordatavalues column:\n",
    "    dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    # Then regroup by cluster, keeping the cluster center and the sensordatavalues for each cluster id:\n",
    "    dfs[i] = dfs[i].groupby('cluster_id').agg(collect_list('cluster_center')[0].alias('cluster_center'), collect_list('sensor_amount')[0].alias('sensor_amount'), collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Once again, just like task 1, we compute the max AQI for each cluster:\n",
    "    # Create a RDD collection of tuples (country, (P1, P2)), so that each rdd element is a combo of a cluster and either its P1 or P2 values. Then convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [float(y['value']) for y in x[3] if y[2] == 'P1'])).toDF(['cluster_id', 'P1']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [float(y['value']) for y in x[3] if y[2] == 'P2'])).toDF(['cluster_id', 'P2']), on='cluster_id', how='inner')\n",
    "    # Create a RDD collection to calculate the average P1 and P2 values for each cluster, and convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], float(np.round(np.mean(x[4]), 2)))).toDF(['cluster_id', 'avgP1']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], float(np.round(np.mean(x[5]), 2)))).toDF(['cluster_id', 'avgP1']), on='cluster_id', how='inner')\n",
    "    # Associate P1 and P2 avg to their respective AQI:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[6]) <= air[y][2][1]][0])).toDF(['cluster_id', 'P1_AQI']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[7]) <= air[y][2][1]][0])).toDF(['cluster_id', 'P2_AQI']), on='cluster_id', how='inner')\n",
    "    # Calculate the max AQI for each cluster:\n",
    "    dfs[i] = dfs[i].withColumn('maxAQI', when(dfs[i].P1_AQI > dfs[i].P2_AQI, dfs[i].P1_AQI).otherwise(dfs[i].P2_AQI))\n",
    "    dfs[i] = dfs[i].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI')\n",
    "    dfs[i].show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To complete task 2, let's compare two of our previously grouped-by-clusters dataframes and table the top 50 clusters whose maxAQI difference is the lowest (the lower the diffAQI is, the merrier the air quality evolution is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Select one dataframe (here the one before the latest):\n",
    "df1 = dfs[len(dfs)-2].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "# Select a second dataframe (here the latest):\n",
    "df2 = dfs[len(dfs)-1].select('cluster_id', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "# Join both dataframes on cluster_id:\n",
    "df_diff = df1.join(df2, on='cluster_id', how='inner')\n",
    "# Create a column named diffAQI, whose value is the relative difference between today's maxAQI, and yesterday maxAQI:\n",
    "df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('cluster_id', 'cluster_center', 'sensor_amount', 'diffAQI')\n",
    "# Sort the dataframe by diffAQI, starting with the lowest diffAQIs:\n",
    "df_diff.sort('diffAQI', ascending=True).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Plot each cluster's center on a world map using geopandas:\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "shapefile = 'data/ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp'\n",
    "\n",
    "colors = 9\n",
    "cmap = 'Blues'\n",
    "figsize = (16, 10)\n",
    "\n",
    "# Create a world map:\n",
    "world = gpd.read_file(shapefile)[['ADMIN', 'ADM0_A3', 'geometry']]\n",
    "world.columns = ['country', 'country_code', 'geometry']\n",
    "# Plot the world map:\n",
    "# ax = world.plot(color='white', edgecolor='black', figsize=figsize)\n",
    "# Plot each cluster's center on the world map:\n",
    "for i in range(len(df_diff.take(50))):\n",
    "    plt.scatter(df_diff.take(50)[i][1][0], df_diff.take(50)[i][1][1], c=df_diff.take(50)[i][3], cmap=cmap, vmin=-1, vmax=1, s=1000, alpha=0.5)\n",
    "# Add a colorbar:\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=-1, vmax=1))\n",
    "sm._A = []\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.set_label('AQI difference', rotation=270, labelpad=20)\n",
    "# Show the world map:\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Calculate the longest streaks of good air quality (ie low index values) and display as a histogram.\n",
    "\n",
    "Assume task 2 is already done. We will use the dataframes where AQI values are already calculated for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a RDD with the cluster id, and a list containing 0s or 1s if the maxAQI is respectively lower or higher than 3:\n",
    "rdd = dfs[0].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1 ]))\n",
    "# Create a RDD with the cluster id, the list of 0s/1s, the current streak of repetitive 0s and the max streak:\n",
    "rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][0] == 0 else 0), (1 if x[1][0] == 0 else 0)))\n",
    "# Convert the RDD to a dataframe:\n",
    "df = rdd_streaks.toDF(['cluster_id', 'streaks', 'current_streak', 'max_streak'])\n",
    "# For all the following days (each dataframes following the first stored one)\n",
    "for i in range(1, len(dfs)):\n",
    "    # Create a RDD with the cluster id, and a list containing 0s or 1s if the maxAQI is respectively lower or higher than 3:\n",
    "    rdd = dfs[i].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1 ]))\n",
    "    # Create a RDD with the cluster id, the list of 0s/1s and the current streak:\n",
    "    rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][len(x[1])-1] == 0 else 0)))\n",
    "    # Convert the RDD containing streak information to a dataframe, and join it to the previous dataframe:\n",
    "    df = df.join(rdd_streaks.toDF(['cluster_id', 'streak', 'previous_streak']), on='cluster_id', how='inner')\n",
    "    # Concatenate the 0s/1s values list with the current df 0s/1s value into a single list, and drop the colomn with only one value:\n",
    "    df = df.withColumn('streaks', concat('streaks', 'streak'))\n",
    "    df = df.drop('streak')\n",
    "    # Update the current_streak column using the previous_streak value:\n",
    "    df = df.withColumn('current_streak', when(df.previous_streak == 1, df.current_streak + 1).otherwise(0))\n",
    "    # Update the max_streak value using the previous_streak and the max_streak:\n",
    "    df = df.withColumn('max_streak', when(df.current_streak > df.max_streak, df.current_streak).otherwise(df.max_streak))\n",
    "    # Drop the current streak value:\n",
    "    df = df.drop('previous_streak')\n",
    "# Show the current state of streaks for each cluster id (for verification):\n",
    "df.sort('cluster_id').show(10)\n",
    "# Group by max_streak, show the cluster_id and the amount of clusters with that max_streak:\n",
    "df = df.groupBy('max_streak').agg(count('cluster_id').alias('cluster_amount'), collect_list('cluster_id').alias('cluster_ids'))\n",
    "df = df.sort('max_streak', ascending=False)\n",
    "print('The clusters ids with the longest streaks of good air quality (ie low index values), as well as the amount of clusters with that streak:')\n",
    "df.show(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create histogram of the streaks:\n",
    "df = df.withColumn('max_streak', df.max_streak.cast('int'))\n",
    "df = df.withColumn('cluster_amount', df.cluster_amount.cast('int'))\n",
    "pdf = df.sort('max_streak', ascending=True).toPandas()\n",
    "pdf.plot.bar(x='max_streak', y='cluster_amount', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
