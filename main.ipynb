{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MLBD : Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TODO: Switch methods from panda to spark when possible\n",
    "### TODO: Work on a way to acquire data multiples times a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spark session builder:\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Locally save instances of the data:\n",
    "import urllib.request, json, datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "now = datetime.datetime.now()\n",
    "url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "with urllib.request.urlopen(url5min) as url:\n",
    "    data = json.load(url)\n",
    "with urllib.request.urlopen(url24h) as url:\n",
    "    data24h = json.load(url)\n",
    "with open('output/data5min_{}_{}h{}.json'.format(today, now.hour, str(now.minute).zfill(2)), 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "with open('output/data24h_{}.json'.format(today), 'w') as outfile:\n",
    "    json.dump(data24h, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from local files and urls into Spark DataFrames:\n",
    "url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "fileToCompare = 'output/data5min_2022-10-31_16h08.json'\n",
    "\n",
    "# 1\n",
    "spark.sparkContext.addFile(url24h)\n",
    "filename24 = SparkFiles.get(url24h.split('/')[-1])\n",
    "\n",
    "df24h = spark.read.json(filename24)\n",
    "df24h.createOrReplaceTempView(\"df24h\")\n",
    "df24h.printSchema()\n",
    "\n",
    "# 2\n",
    "spark.sparkContext.addFile(url5min)\n",
    "filename5 = SparkFiles.get(url5min.split('/')[-1])\n",
    "\n",
    "df5min = spark.read.json(filename5)\n",
    "df5min.createOrReplaceTempView(\"df5min\")\n",
    "df5min.printSchema()\n",
    "\n",
    "# 3: create spark dataframe using local json file (for comparison)\n",
    "spark.sparkContext.addFile(fileToCompare)\n",
    "filename5_2 = SparkFiles.get(fileToCompare.split('/')[-1])\n",
    "\n",
    "df5min2 = spark.read.json(filename5_2)\n",
    "df5min2.createOrReplaceTempView(\"df5min2\")\n",
    "df5min2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Spark dataframes used for tasks :\n",
    "dfs = [df5min, df5min2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark implementation & tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove sensor values whose sensor value_type is not 'P1' or 'P2':\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = dfs[i].\\\n",
    "        where(array_contains(col('sensordatavalues.value_type'), 'P1')\n",
    "              | array_contains(col('sensordatavalues.value_type'),'P2'))\n",
    "\n",
    "    # Remove everything but P1 and P2 in df, by creating a new df using pandas and exploding the sensordatavalues column:\n",
    "    pdf = dfs[i].toPandas()\n",
    "    pdf = pdf.explode('sensordatavalues')\n",
    "    pdf = pdf[pdf['sensordatavalues'].apply(lambda x: x['value_type'] in ['P1', 'P2'])]\n",
    "    pdf = pdf.groupby('id').agg({'sensordatavalues': lambda x: list(x)})\n",
    "    pdf['id'] = pdf.index\n",
    "\n",
    "    df_sdv = spark.createDataFrame(pdf)\n",
    "\n",
    "    dfs[i] = dfs[i].drop('sensordatavalues')\n",
    "    dfs[i] = dfs[i].join(df_sdv, on='id')\n",
    "\n",
    "    dfs[i].select('id','location.country','location.id', 'sensordatavalues.value_type','sensordatavalues.value').sort('location.country', 'location.id').show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Explode the sensordatavalues column, and make it so that P1 and P2 are in individual rows, for a given country:\n",
    "for i in range(len(dfs)):\n",
    "    pdf = dfs[i].toPandas()\n",
    "    pdf['location'] = pdf['location'].apply(lambda x: x[1])\n",
    "    pdf['sensor'] = pdf['sensor'].apply(lambda x: x[0])\n",
    "    pdf = pdf.sort_values(by=['location'])\n",
    "    pdf = pdf.explode('sensordatavalues')\n",
    "    pdf = pdf.groupby('location').agg({'location': lambda x: list(x)[0], 'sensor': lambda x: list(x), 'sensordatavalues': lambda x: list(x)})\n",
    "    pdf['sensordatavalues'] = pdf.apply(lambda x: [[str(x['sensor'][i]), str(x['sensordatavalues'][i][1]), x['sensordatavalues'][i][2]] for i in range(len(x['sensor']))], axis=1)\n",
    "    pdf['P1'] = pdf['sensordatavalues'].apply(lambda x: [float(i[1]) for i in x if i[2] == 'P1'])\n",
    "    pdf['P2'] = pdf['sensordatavalues'].apply(lambda x: [float(i[1]) for i in x if i[2] == 'P2'])\n",
    "    pdf = pdf.drop(['sensor'], axis=1)\n",
    "    sdf = spark.createDataFrame(pdf)\n",
    "    dfs[i] = sdf\n",
    "    dfs[i].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a map to store air quality indexes: Bulk method\n",
    "air = {}\n",
    "air[1] = [\"Low\", [0,16], [0,11]]\n",
    "air[2] = [\"Low\", [17,33], [12,23]]\n",
    "air[3] = [\"Low\", [34,50], [24,35]]\n",
    "air[4] = [\"Medium\", [51,58], [36,41]]\n",
    "air[5] = [\"Medium\", [59,66], [42,47]]\n",
    "air[6] = [\"Medium\", [67,75], [48,53]]\n",
    "air[7] = [\"High\", [76,83], [54,58]]\n",
    "air[8] = [\"High\", [84,91], [59,64]]\n",
    "air[9] = [\"High\", [92,100], [65,70]]\n",
    "air[10] = [\"Very High\", [101,10000000], [71,10000000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the average air quality index for each country, and compare it to the previous 24 hours:\n",
    "pdf1 = dfs[0].toPandas()\n",
    "pdf2 = dfs[1].toPandas()\n",
    "\n",
    "pdf1['meanP1'] = pdf1['P1'].apply(lambda x: np.mean(x))\n",
    "pdf1['meanP2'] = pdf1['P2'].apply(lambda x: np.mean(x))\n",
    "pdf1['AQI_P1'] = pdf1['meanP1'].apply(lambda x: [i for i in air if air[i][1][0] <= round(x) <= air[i][1][1]][0])\n",
    "pdf1['AQI_P2'] = pdf1['meanP2'].apply(lambda x: [i for i in air if air[i][2][0] <= round(x) <= air[i][2][1]][0])\n",
    "pdf1['maxAQI'] = pdf1.apply(lambda x: max(x['AQI_P1'], x['AQI_P2']), axis=1)\n",
    "pdf1.drop(['sensordatavalues','P1', 'P2', 'meanP1', 'meanP2', 'AQI_P1', 'AQI_P2'], axis=1, inplace=True)\n",
    "\n",
    "pdf2['meanP1'] = pdf2['P1'].apply(lambda x: np.mean(x))\n",
    "pdf2['meanP2'] = pdf2['P2'].apply(lambda x: np.mean(x))\n",
    "pdf2['AQI_P1'] = pdf2['meanP1'].apply(lambda x: [i for i in air if air[i][1][0] <= round(x) <= air[i][1][1]][0])\n",
    "pdf2['AQI_P2'] = pdf2['meanP2'].apply(lambda x: [i for i in air if air[i][2][0] <= round(x) <= air[i][2][1]][0])\n",
    "pdf2['maxAQI'] = pdf2.apply(lambda x: max(x['AQI_P1'], x['AQI_P2']), axis=1)\n",
    "pdf2.drop(['sensordatavalues','P1', 'P2', 'meanP1', 'meanP2', 'AQI_P1', 'AQI_P2'], axis=1, inplace=True)\n",
    "\n",
    "pdf = pdf1.merge(pdf2, on='location', how='outer', suffixes=('_1', '_2'))\n",
    "# remove rows with NaN values: We consider that no new measure means that the air quality did not improve, nor worsen :\n",
    "pdf = pdf.dropna()\n",
    "# convert to int\n",
    "pdf['maxAQI_1'] = pdf['maxAQI_1'].astype(int)\n",
    "pdf['maxAQI_2'] = pdf['maxAQI_2'].astype(int)\n",
    "pdf['AQI_diff'] = pdf.apply(lambda x: x['maxAQI_2'] - x['maxAQI_1'], axis=1)\n",
    "pdf = pdf.sort_values(by=['AQI_diff'], ascending=True)\n",
    "pdf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
