{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MSc in CSTE, CIDA option Machine learning & Big Data Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Analysis of data from an environmental sensor network using Hadoop/Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start a timer:\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import *\n",
    "import urllib.request, json, datetime\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from itertools import groupby\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import branca\n",
    "from folium.plugins import HeatMap\n",
    "import folium\n",
    "import altair as alt\n",
    "import pycountry as pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SETTINGS=\n",
    "MAX_MEMORY= \"8g\"\n",
    "\n",
    "# Spark session builder:\n",
    "spark = SparkSession.builder.config(\"spark.executor.memory\", MAX_MEMORY).config(\"spark.driver.memory\", MAX_MEMORY).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"OFF\")\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Locally save instances of the data:\n",
    "def save_data(wait):\n",
    "    # URLs of the data:\n",
    "    url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "    url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "    # If wait=True, sleep until specific time (e.g. 5pm) before running the next line\n",
    "    if wait:\n",
    "        today = datetime.datetime.now()\n",
    "        exactImportTime = datetime.datetime(today.year, today.month, today.day, 17, 0, 0)\n",
    "        awaitingTime = exactImportTime - today\n",
    "        time.sleep(awaitingTime.total_seconds())\n",
    "\n",
    "    # Download the data, saved as json files:\n",
    "    today = datetime.datetime.now()\n",
    "    with urllib.request.urlopen(url5min) as url:\n",
    "        data5min = json.load(url)\n",
    "    with open('output/5min/data5min_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "        json.dump(data5min, outfile)\n",
    "    with urllib.request.urlopen(url24h) as url:\n",
    "        data24h = json.load(url)\n",
    "    with open('output/24h/data24h_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "        json.dump(data24h, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from local files and load them into Spark DataFrames:\n",
    "def load_data(all):\n",
    "    path = 'output/24h/'\n",
    "    file1 = 'data24h_2022-11-3_17h00.json'\n",
    "    file2 = 'data24h_2022-11-4_17h00.json'\n",
    "    file3 = 'data24h_2022-11-5_17h00.json'\n",
    "    file4 = 'data24h_2022-11-6_17h00.json'\n",
    "    file5 = 'data24h_2022-11-7_17h00.json'\n",
    "    file6 = 'data24h_2022-11-8_17h00.json'\n",
    "    file7 = 'data24h_2022-11-9_17h00.json'\n",
    "    file8 = 'data24h_2022-11-10_17h00.json'\n",
    "    file9 = 'data24h_2022-11-11_17h00.json'\n",
    "    file10 = 'data24h_2022-11-12_17h00.json'\n",
    "    file11 = 'data24h_2022-11-13_17h00.json'\n",
    "    file12 = 'data24h_2022-11-14_17h00.json'\n",
    "    file13 = 'data24h_2022-11-15_17h00.json'\n",
    "    file14 = 'data24h_2022-11-16_17h00.json'\n",
    "    file15 = 'data24h_2022-11-17_17h00.json'\n",
    "    file16 = 'data24h_2022-11-18_17h00.json'\n",
    "    file17 = 'data24h_2022-11-19_17h00.json'\n",
    "    file18 = 'data24h_2022-11-20_17h00.json'\n",
    "    file19 = 'data24h_2022-11-21_17h00.json'\n",
    "    file20 = 'data24h_2022-11-22_17h00.json'\n",
    "\n",
    "    files = [file1, file2, file3, file4, file5, file6, file7, file8, file9, file10, file11, file12, file13, file14, file15, file16, file17, file18, file19, file20]\n",
    "    dfs = []\n",
    "\n",
    "    if all:\n",
    "        for file in files:\n",
    "            spark.sparkContext.addFile(path + file)\n",
    "            filename = SparkFiles.get(file)\n",
    "            df = spark.read.json(filename)\n",
    "            dfs.append(df)\n",
    "    else:\n",
    "        for file in files[len(files)-2:]:\n",
    "            spark.sparkContext.addFile(path + file)\n",
    "            filename = SparkFiles.get(file)\n",
    "            df = spark.read.json(filename)\n",
    "            dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark implementation & tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 0: Data pre-processing, filtering and cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing (P1 and P2 grouping) (Common to all tasks, the returned dataframe will be used for all three tasks):\n",
    "def preprocessing(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        # Remove columns that are not needed for all three tasks:\n",
    "        dfs[i] = dfs[i].drop('sampling_rate', 'timestamp').withColumn('country', dfs[i].location.country).withColumn('latitude', dfs[i].location.latitude.cast('float')).withColumn('longitude', dfs[i].location.longitude.cast('float')).withColumn('sensor_id', dfs[i].sensor.id).drop('location', 'sensor')\n",
    "        # Explode sensordatavalues using pyspark.sql.functions.explode\n",
    "        df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # Remove rows that aren't P1 or P2:\n",
    "        df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "        # Remove rows that have negative values:\n",
    "        df_ = df_[df_.sensordatavalues.value >= 0]\n",
    "        # Regroup sensordatavalues by record id:\n",
    "        df_ = df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "        # Remove the old sensordatavalues column still containing values different from P1 and P2:\n",
    "        dfs[i] = dfs[i].drop('sensordatavalues')\n",
    "        # Link the new sensordatavalues column to the old dataframe, on id:\n",
    "        dfs[i] = dfs[i].join(df_, on='id', how='inner')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def group_by_country(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        # Explode sensordatavalues column:\n",
    "        dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # Create a column only for P1 and P2 values:\n",
    "        dfs[i] = dfs[i].withColumn('P1', when(dfs[i].sensordatavalues.value_type == 'P1', dfs[i].sensordatavalues.value).otherwise(None))\n",
    "        dfs[i] = dfs[i].withColumn('P2', when(dfs[i].sensordatavalues.value_type == 'P2', dfs[i].sensordatavalues.value).otherwise(None))\n",
    "        # Group by country and calculate the mean of P1 and P2 for each cluster, keeping the latitude and longitude:\n",
    "        dfs[i] = dfs[i].groupBy('country').agg(avg('latitude').alias('latitude'), avg('longitude').alias('longitude'), avg('P1').alias('avgP1'), avg('P2').alias('avgP2'))\n",
    "        # Round latitude and longitude to 2 decimals:\n",
    "        dfs[i] = dfs[i].withColumn('latitude', round(dfs[i].latitude, 2)).withColumn('longitude', round(dfs[i].longitude, 2))\n",
    "        # Map the average P1 and P2 values to the AQI scale using when statements:\n",
    "        dfs[i] = dfs[i].withColumn('avgP1_AQI', when(dfs[i].avgP1 < 17, 1).otherwise(\n",
    "            when(dfs[i].avgP1 < 34, 2).otherwise(\n",
    "                when(dfs[i].avgP1 < 51, 3).otherwise(\n",
    "                    when(dfs[i].avgP1 < 59, 4).otherwise(\n",
    "                        when(dfs[i].avgP1 < 67, 5).otherwise(\n",
    "                            when(dfs[i].avgP1 < 76, 6).otherwise(\n",
    "                                when(dfs[i].avgP1 < 84, 7).otherwise(\n",
    "                                    when(dfs[i].avgP1 < 92, 8).otherwise(\n",
    "                                        when(dfs[i].avgP1 < 101, 9).otherwise(10))))))))))\n",
    "        dfs[i] = dfs[i].withColumn('avgP2_AQI', when(dfs[i].avgP2 < 12, 1).otherwise(\n",
    "            when(dfs[i].avgP2 < 24, 2).otherwise(\n",
    "                when(dfs[i].avgP2 < 36, 3).otherwise(\n",
    "                    when(dfs[i].avgP2 < 42, 4).otherwise(\n",
    "                        when(dfs[i].avgP2 < 48, 5).otherwise(\n",
    "                            when(dfs[i].avgP2 < 54, 6).otherwise(\n",
    "                                when(dfs[i].avgP2 < 59, 7).otherwise(\n",
    "                                    when(dfs[i].avgP2 < 65, 8).otherwise(\n",
    "                                        when(dfs[i].avgP2 < 71, 9).otherwise(10))))))))))\n",
    "        # Calculate the max AQI value for each cluster:\n",
    "        dfs[i] = dfs[i].withColumn('maxAQI', when(dfs[i].avgP1_AQI > dfs[i].avgP2_AQI, dfs[i].avgP1_AQI).otherwise(dfs[i].avgP2_AQI))\n",
    "        dfs[i] = dfs[i].select('country', 'latitude', 'longitude', 'maxAQI')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task1(dfs):\n",
    "    # Choosing the last two dataframes, identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each.\n",
    "    df1 = dfs[len(dfs)-2].select('country', 'latitude', 'longitude', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "    df2 = dfs[len(dfs)-1].select('country', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "    df_diff = df1.join(df2, on='country', how='inner')\n",
    "    df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('country', 'latitude', 'longitude', 'diffAQI')\n",
    "    df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Folium map of top 10 countries:\n",
    "def task1_map(df_diff):\n",
    "    # Create a map of the top 10 countries:\n",
    "    map = folium.Map(location=[30,0], zoom_start=3.2, tiles='Stamen Terrain', width='100%', height='100%')\n",
    "    # Feature group for the top 10 countries:\n",
    "    top10 = folium.FeatureGroup(name='Top 10 countries with AQI improvement')\n",
    "    # Feature group for the rest of the countries:\n",
    "    rest = folium.FeatureGroup(name='Rest of the countries')\n",
    "    # Add a marker for each country:\n",
    "    for i in range(df_diff.count()):\n",
    "        # Associate the full country name to the country code:\n",
    "        fullnamecountry = pycountry.countries.get(alpha_2=df_diff.collect()[i].country).name\n",
    "        # Popup the full name country and the AQI difference over the last 24 hours:\n",
    "        popup = folium.Popup('<b>Country:</b> ' + fullnamecountry + '<br><b>AQI difference:</b> ' + str(df_diff.collect()[i].diffAQI), max_width=200)\n",
    "        lat = df_diff.collect()[i]['latitude']\n",
    "        lon = df_diff.collect()[i]['longitude']\n",
    "        # Add the marker to the top10 feature group if it is in the top 10:\n",
    "        if i < 10:\n",
    "            top10.add_child(folium.Marker(location=[lat, lon], popup=popup, icon=folium.Icon(color='green')))\n",
    "        # Add the marker to the rest feature group if it is not in the top 10:\n",
    "        else:\n",
    "            rest.add_child(folium.Marker(location=[lat, lon], popup=popup, icon=folium.Icon(color='red')))\n",
    "        #NB: This if(i<10) works because the dataframe is sorted in ascending order, so the first 10 countries are the ones with the highest AQI improvement.\n",
    "    # Add the feature groups to the map:\n",
    "    map.add_child(top10)\n",
    "    map.add_child(rest)\n",
    "    # Add a layer control to the map:\n",
    "    map.add_child(folium.map.LayerControl())\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 2: Using the geo-coordinates from the sensor data, group the data into smaller regions using an appropriate clustering algorithm. Then determine the top 50 regions in terms of air quality improvement over the previous 24 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2-1: Pre-filter the pre-processed data by creating clusters and grouping data by cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def group_by_cluster(dfs, amount):\n",
    "    # Create a KMeans model fitting on the first dataframe, using the latitude and logitude columns, combined together in a features column used to predict the future coordinates:\n",
    "    model = KMeans(k=amount, seed=23, featuresCol='features', predictionCol='cluster_id').fit(VectorAssembler(inputCols=['latitude', 'longitude'], outputCol='features').transform(dfs[0]))\n",
    "    # (This model will be used to predict the cluster for each sensor id in the second dataframe.)\n",
    "    for i in range(len(dfs)):\n",
    "        # Transform the dataframe:\n",
    "        dfs[i] = VectorAssembler(inputCols=['latitude', 'longitude'], outputCol='features').transform(dfs[i])\n",
    "        # Predict the cluster for each sensor id:\n",
    "        dfs[i] = model.transform(dfs[i])\n",
    "        # Explode sensordatavalues column:\n",
    "        dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "        # Create a column only for P1 and P2 values:\n",
    "        dfs[i] = dfs[i].withColumn('P1', when(dfs[i].sensordatavalues.value_type == 'P1', dfs[i].sensordatavalues.value).otherwise(None))\n",
    "        dfs[i] = dfs[i].withColumn('P2', when(dfs[i].sensordatavalues.value_type == 'P2', dfs[i].sensordatavalues.value).otherwise(None))\n",
    "        # Create a column for the coordinates, an array of latitude and longitude:\n",
    "        dfs[i] = dfs[i].withColumn('coordinates', array('latitude', 'longitude'))\n",
    "        # Group by cluster_id and calculate the mean of P1 and P2 for each cluster, and keep the sensor_amount and a list of sensor_ids as well as the coordinates:\n",
    "        dfs[i] = dfs[i].groupBy('cluster_id').agg(mean('P1').alias('avgP1'), mean('P2').alias('avgP2'), count('sensor_id').alias('sensor_amount'), collect_list('sensor_id').alias('sensor_ids'), collect_list('coordinates').alias('coordinates'))\n",
    "\n",
    "        # Store each cluster's center in the dataframe:\n",
    "        cluster_centers = []\n",
    "        for k in range(len(model.clusterCenters())):\n",
    "            cluster_centers.append([float(np.round(model.clusterCenters()[k][0], 2)), float(np.round(model.clusterCenters()[k][1], 2))])\n",
    "        cluster_ids = [i for i in range(len(cluster_centers))]\n",
    "        # Create a dataframe containing the cluster_id and the cluster_center:\n",
    "        df_cluster_centers = spark.createDataFrame(list(zip(cluster_ids, cluster_centers)), ['cluster_id', 'cluster_center'])\n",
    "        # Join the two dataframes:\n",
    "        dfs[i] = dfs[i].join(df_cluster_centers, on='cluster_id', how='inner')\n",
    "\n",
    "        # Map the average P1 and P2 values to the AQI scale using when statements:\n",
    "        dfs[i] = dfs[i].withColumn('avgP1_AQI', when(dfs[i].avgP1 < 17, 1).otherwise(\n",
    "            when(dfs[i].avgP1 < 34, 2).otherwise(\n",
    "                when(dfs[i].avgP1 < 51, 3).otherwise(\n",
    "                    when(dfs[i].avgP1 < 59, 4).otherwise(\n",
    "                        when(dfs[i].avgP1 < 67, 5).otherwise(\n",
    "                            when(dfs[i].avgP1 < 76, 6).otherwise(\n",
    "                                when(dfs[i].avgP1 < 84, 7).otherwise(\n",
    "                                    when(dfs[i].avgP1 < 92, 8).otherwise(\n",
    "                                        when(dfs[i].avgP1 < 101, 9).otherwise(10))))))))))\n",
    "        dfs[i] = dfs[i].withColumn('avgP2_AQI', when(dfs[i].avgP2 < 12, 1).otherwise(\n",
    "            when(dfs[i].avgP2 < 24, 2).otherwise(\n",
    "                when(dfs[i].avgP2 < 36, 3).otherwise(\n",
    "                    when(dfs[i].avgP2 < 42, 4).otherwise(\n",
    "                        when(dfs[i].avgP2 < 48, 5).otherwise(\n",
    "                            when(dfs[i].avgP2 < 54, 6).otherwise(\n",
    "                                when(dfs[i].avgP2 < 59, 7).otherwise(\n",
    "                                    when(dfs[i].avgP2 < 65, 8).otherwise(\n",
    "                                        when(dfs[i].avgP2 < 71, 9).otherwise(10))))))))))\n",
    "        # Calculate the max AQI value for each cluster:\n",
    "        dfs[i] = dfs[i].withColumn('maxAQI', when(dfs[i].avgP1_AQI > dfs[i].avgP2_AQI, dfs[i].avgP1_AQI).otherwise(dfs[i].avgP2_AQI))\n",
    "        dfs[i] = dfs[i].select('cluster_id', 'cluster_center', 'sensor_amount', 'sensor_ids', 'coordinates', 'maxAQI').sort('cluster_id')\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2-2: Select last two dataframes and compare their AQI to sort clusters by the AQI difference over the last 24 hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task2(dfs):\n",
    "    # Select the last two dataframes, to compare the evolution of the air quality between the last 24 hours:\n",
    "    df1 = dfs[len(dfs)-2].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "    df2 = dfs[len(dfs)-1].select('cluster_id', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "    # Join both dataframes on cluster_id:\n",
    "    df_diff = df1.join(df2, on='cluster_id', how='inner')\n",
    "    # Create a column named diffAQI, whose value is the relative difference between today's maxAQI, and yesterday maxAQI:\n",
    "    df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('cluster_id', 'cluster_center', 'sensor_amount', 'diffAQI')\n",
    "    # Sort the dataframe by diffAQI, starting with the lowest diffAQIs:\n",
    "    df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task2_map(df_diff):\n",
    "    df_diff_collect = df_diff.collect()\n",
    "    # Create a map of the clusters, with the AQI difference as the color:\n",
    "    map = folium.Map(location=[30,0], zoom_start=3.2, tiles='Stamen Terrain', width='100%', height='100%')\n",
    "    # Create a feature group:\n",
    "    improvedAqi = folium.FeatureGroup(name='Top 50 AQI improved clusters')\n",
    "    worsenedAqi = folium.FeatureGroup(name='The ' + str(df_diff.count()) + ' other clusters')\n",
    "    colors = ['green', 'red']\n",
    "    # Add a marker for each cluster:\n",
    "    for i in range(df_diff.count()):\n",
    "        # Get the cluster's coordinates:\n",
    "        coordinates = df_diff_collect[i][1]\n",
    "        # Get the AQI difference:\n",
    "        diffAQI = df_diff_collect[i].diffAQI\n",
    "        # Create a popup with the cluster's id, the AQI difference, and the number of sensors in the cluster:\n",
    "        popup = folium.Popup('<b>Cluster id: </b>' + str(df_diff_collect[i].cluster_id) + '<br><b>AQI difference: </b>' + str(diffAQI) + '<br><b>Sensors: </b>' + str(df_diff_collect[i].sensor_amount), max_width=450)\n",
    "        # Add a marker to the map:\n",
    "        if i < 50:\n",
    "            worsenedAqi.add_child(folium.CircleMarker(location=[coordinates[0], coordinates[1]], radius=7, popup=popup, fill_color=colors[0], color='black', fill_opacity=0.7))\n",
    "        else:\n",
    "            improvedAqi.add_child(folium.CircleMarker(location=[coordinates[0], coordinates[1]], radius=7, popup=popup, fill_color=colors[1], color='black', fill_opacity=0.7))\n",
    "    # Add the feature groups to the map:\n",
    "    map.add_child(improvedAqi)\n",
    "    map.add_child(worsenedAqi)\n",
    "    # Add a layer control:\n",
    "    map.add_child(folium.map.LayerControl())\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Calculate the longest streaks of good air quality (ie low index values) and display as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task3(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i] = dfs[i].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI').sort('cluster_id')\n",
    "    # Create a RDD with the cluster id, and a list containing 0s or 1s if the maxAQI is respectively lower or higher than 3:\n",
    "    rdd = dfs[0].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1]))\n",
    "    # Create a RDD with the cluster id, the list of 0s/1s, the current streak of repetitive 0s and the max streak:\n",
    "    rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][0] == 0 else 0), (1 if x[1][0] == 0 else 0)))\n",
    "    # Convert the RDD to a dataframe:\n",
    "    df = rdd_streaks.toDF(['cluster_id', 'streaks', 'current_streak', 'max_streak'])\n",
    "    # For all the following days (each dataframes following the first stored one)\n",
    "    for i in range(1, len(dfs)):\n",
    "        # Create a RDD with the cluster id, and a list containing 0s or 1s if the maxAQI is respectively lower or higher than 3:\n",
    "        rdd = dfs[i].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1]))\n",
    "        # Create a RDD with the cluster id, the list of 0s/1s and the current streak:\n",
    "        rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][len(x[1])-1] == 0 else 0)))\n",
    "        # Convert the RDD containing streak information to a dataframe, and join it to the previous dataframe:\n",
    "        df = df.join(rdd_streaks.toDF(['cluster_id', 'streak', 'previous_streak']), on='cluster_id', how='inner')\n",
    "        # Concatenate the 0s/1s values list with the current df 0s/1s value into a single list, and drop the colomn with only one value:\n",
    "        df = df.withColumn('streaks', concat('streaks', 'streak'))\n",
    "        df = df.drop('streak')\n",
    "        # Update the current_streak column using the previous_streak value:\n",
    "        df = df.withColumn('current_streak', when(df.previous_streak == 1, df.current_streak + 1).otherwise(0))\n",
    "        # Update the max_streak value using the previous_streak and the max_streak:\n",
    "        df = df.withColumn('max_streak', when(df.current_streak > df.max_streak, df.current_streak).otherwise(df.max_streak))\n",
    "        # Drop the current streak value:\n",
    "        df = df.drop('previous_streak')\n",
    "    # Show the current state of streaks for each cluster id (for verification):\n",
    "    df = df.sort('cluster_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Two histogram methods:\n",
    "Method 1: A single histogram showing longest (maximum or average) streaks across all regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def method_one_histogram(df):\n",
    "    # Group by max_streak, show clusters ids with such max streak, as well as their amount:\n",
    "    df_ = df.groupBy('max_streak').agg(count('cluster_id').alias('cluster_amount'), collect_list('cluster_id').alias('cluster_ids'))\n",
    "    # Cast the max_streak and cluster_amount columns to int:\n",
    "    df_ = df_.withColumn('max_streak', df_.max_streak.cast('int')).withColumn('cluster_amount', df_.cluster_amount.cast('int'))\n",
    "    # Sort the dataframe by max_streak:\n",
    "    df_ = df_.sort('max_streak', ascending=False)\n",
    "    pdf = df_.toPandas()\n",
    "    pdf.plot.bar(x='max_streak', y='cluster_amount', rot=0)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Method 2: A histogram for each region/cluster, showing the distribution of continuous good AQI streaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def method_two_histogram(df):\n",
    "    # Calculate successive 0s for each cluster:\n",
    "    pdf = df.toPandas()\n",
    "    max_streak = len(dataframes)\n",
    "    # Create a list of lists, where each list contains the successive 0s for each cluster:\n",
    "    pdf['successive_0s'] = pdf['streaks'].apply(lambda x: [len(list(g)) for k, g in groupby(x) if k == 0])\n",
    "    # Count the amount of clusters with a certain amount of successive 0s:\n",
    "    pdf['successive_0s_hist'] = pdf['successive_0s'].apply(lambda x: [x.count(i) for i in range(1, max_streak+1)])\n",
    "    # Deduce the amount of time the cluster didn't came back to a good air quality:\n",
    "    pdf['successive_0s_hist'] = pdf.apply(lambda x: [max_streak - np.sum(x['successive_0s'])] + x['successive_0s_hist'], axis=1)\n",
    "    # Remove useless columns:\n",
    "    pdf = pdf.drop('streaks', axis=1).drop('current_streak', axis=1).drop('max_streak', axis=1)\n",
    "    # Gather back the last day of data, grouped by clusters, in order to get the coordinates of the clusters:\n",
    "    df_fbcr = dfs_fbcr[-1]\n",
    "    # Split cluster center coordinates into two columns, and drop the old cluster_center column:\n",
    "    df_fbcr = df_fbcr.withColumn('center_latitude', df_fbcr['cluster_center'][0]).withColumn('center_longitude', df_fbcr['cluster_center'][1]).drop('cluster_center')\n",
    "    # Convert the dataframe to a pandas dataframe:\n",
    "    pdf_fbcr = df_fbcr.toPandas()\n",
    "    # Inner join the two dataframes:\n",
    "    pdf = pdf_fbcr.merge(pdf, on='cluster_id', how='inner')\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: World map showing the regions with the longest streaks, using the second method's histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def task3_map(df2):\n",
    "    # Create a Folium World Map:\n",
    "    map = folium.Map(location=[30,0], zoom_start=3.2, tiles='Stamen Terrain', width='100%', height='100%')\n",
    "\n",
    "    # Default markers:\n",
    "    markers = folium.FeatureGroup(name='markers')\n",
    "    for i in range(0, len(df2)):\n",
    "        chart = alt.Chart(pd.DataFrame({'Streak length': range(0, len(dataframes)+1), 'Streak occurrences': df2.iloc[i]['successive_0s_hist']})).mark_bar(size=5).encode(\n",
    "            x='Streak length',\n",
    "            y='Streak occurrences',\n",
    "        )\n",
    "        chart_json = json.loads(chart.to_json())\n",
    "        folium.Marker(\n",
    "            location=[df2.iloc[i]['center_latitude'], df2.iloc[i]['center_longitude']],\n",
    "            popup=folium.Popup(max_width=450).add_child(\n",
    "                folium.VegaLite(chart_json, width=300, height=250))\n",
    "        ).add_to(markers)\n",
    "    markers.add_to(map)\n",
    "\n",
    "    # AQI heatmap:\n",
    "    heatmapgroup = folium.FeatureGroup(name='heatmap')\n",
    "    heatmap = HeatMap(list(zip(df2['center_latitude'], df2['center_longitude'], df2['maxAQI'])),\n",
    "                      min_opacity=0.2,\n",
    "                      radius=17, blur=15,\n",
    "                      max_zoom=1,\n",
    "                      gradient={0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'})\n",
    "    heatmap.add_to(heatmapgroup)\n",
    "    heatmapgroup.add_to(map)\n",
    "\n",
    "    # Branca colormap:\n",
    "    colormap = branca.colormap.LinearColormap(\n",
    "        colors=['blue', 'lime', 'orange', 'red'],\n",
    "        vmin=1,\n",
    "        vmax=10\n",
    "    ).to_step(index=[i for i in range(1,11)])\n",
    "    colormap.caption = 'Air quality index'\n",
    "    colormap.add_to(map)\n",
    "\n",
    "    # Add a layer control to the map:\n",
    "    folium.LayerControl().add_to(map)\n",
    "\n",
    "    return map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PROGRAM EXECUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load and pre-process the data:\n",
    "dataframes = preprocessing(load_data(True))\n",
    "\n",
    "# Task 1:\n",
    "df_tsk1 = task1(group_by_country(dataframes[-2:]))\n",
    "\n",
    "# Task 2:\n",
    "df_tsk2 = task2(group_by_cluster(dataframes[-2:], 100))\n",
    "\n",
    "# Task 3:\n",
    "dfs_fbcr = group_by_cluster(dataframes[:], 50)\n",
    "df_tsk3 = task3(dfs_fbcr[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TASK 1 - RESULTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Task 1 - Results through a table & a Folium map:\n",
    "df_tsk1.show(10, False)\n",
    "map_tsk1 = task1_map(df_tsk1)\n",
    "map_tsk1    # Show the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TASK 2 - RESULTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Task 2 - Results through a table & a Folium map:\n",
    "df_tsk2.show(50)\n",
    "map_tsk2 = task2_map(df_tsk2)\n",
    "map_tsk2    # Show the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TASK 3 - RESULTS:\n",
    "Method 1: A single histogram showing longest (maximum or average) streaks across all regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Task 3 - Results through a table & a histogram:\n",
    "df_tsk3.show(50)\n",
    "df1 = method_one_histogram(df_tsk3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Method 2: A histogram for each region/cluster, showing the distribution of continuous good AQI streaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Task 3 - Final dataframe used for the clusters mapping and histograms:\n",
    "df2 = method_two_histogram(df_tsk3)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Task 3 - Results through a Folium map:\n",
    "map_tsk3 = task3_map(df2)\n",
    "map_tsk3    # Show the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Stop the timer and the spark session:\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
