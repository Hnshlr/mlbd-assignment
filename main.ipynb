{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MSc in CSTE, CIDA option Machine learning & Big Data Assignment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of data from an environmental sensor network using Hadoop/Spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import *\n",
    "import urllib.request, json, datetime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 19:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": "'http://rn-dynamic-190-059.wless.cranfield.ac.uk:4040'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark session builder:\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Locally save instances of the data:\n",
    "import time\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "url5min = 'https://data.sensor.community/static/v2/data.json'\n",
    "url24h = 'https://data.sensor.community/static/v2/data.24h.json'\n",
    "\n",
    "# sleep until specific time (e.g. 5pm) before running the next line\n",
    "today = datetime.datetime.now()\n",
    "exactImportTime = datetime.datetime(today.year, today.month, today.day, 17, 0, 0)\n",
    "awaitingTime = exactImportTime - today\n",
    "time.sleep(awaitingTime.total_seconds())\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "with urllib.request.urlopen(url5min) as url:\n",
    "    data5min = json.load(url)\n",
    "with open('output/5min/data5min_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "    json.dump(data5min, outfile)\n",
    "with urllib.request.urlopen(url24h) as url:\n",
    "    data24h = json.load(url)\n",
    "with open('output/24h/data24h_{}-{}-{}_{}h{}.json'.format(today.year, today.month, today.day, today.hour, str(today.minute).zfill(2)), 'w') as outfile:\n",
    "    json.dump(data24h, outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- altitude: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- exact_location: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- indoor: long (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |-- sampling_rate: long (nullable = true)\n",
      " |-- sensor: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- pin: string (nullable = true)\n",
      " |    |-- sensor_type: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- manufacturer: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- sensordatavalues: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- value_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- altitude: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- exact_location: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- indoor: long (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |-- sampling_rate: long (nullable = true)\n",
      " |-- sensor: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- pin: string (nullable = true)\n",
      " |    |-- sensor_type: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- manufacturer: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- sensordatavalues: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- value_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- altitude: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- exact_location: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- indoor: long (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |-- sampling_rate: long (nullable = true)\n",
      " |-- sensor: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- pin: string (nullable = true)\n",
      " |    |-- sensor_type: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- manufacturer: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- sensordatavalues: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- value_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- altitude: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- exact_location: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- indoor: long (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |-- sampling_rate: long (nullable = true)\n",
      " |-- sensor: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- pin: string (nullable = true)\n",
      " |    |-- sensor_type: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- manufacturer: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- sensordatavalues: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- value_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data from local files and load them into Spark DataFrames:\n",
    "path = 'output/24h/'\n",
    "file1 = 'data24h_2022-11-3_17h00.json'\n",
    "file2 = 'data24h_2022-11-4_17h00.json'\n",
    "file3 = 'data24h_2022-11-5_17h00.json'\n",
    "file4 = 'data24h_2022-11-6_17h00.json'\n",
    "\n",
    "files = [file1, file2, file3, file4]\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    spark.sparkContext.addFile(path + file)\n",
    "    filename = SparkFiles.get(file)\n",
    "    df = spark.read.json(filename)\n",
    "    df.printSchema()\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# AQI Map:\n",
    "air = {}\n",
    "air[1] = [\"Low\", [0,16], [0,11]]\n",
    "air[2] = [\"Low\", [17,33], [12,23]]\n",
    "air[3] = [\"Low\", [34,50], [24,35]]\n",
    "air[4] = [\"Medium\", [51,58], [36,41]]\n",
    "air[5] = [\"Medium\", [59,66], [42,47]]\n",
    "air[6] = [\"Medium\", [67,75], [48,53]]\n",
    "air[7] = [\"High\", [76,83], [54,58]]\n",
    "air[8] = [\"High\", [84,91], [59,64]]\n",
    "air[9] = [\"High\", [92,100], [65,70]]\n",
    "air[10] = [\"Very High\", [101,10000000], [71,10000000]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spark implementation & tasks:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset count:  25282\n",
      "Dataset count after exploding sensordatavalues:  62869\n",
      "Dataset count after removing rows that aren't P1 or P2:  26374\n",
      "Dataset count after joining the new sensordatavalues column to the old dataframe:  13194\n",
      "+-----------+--------------------+-------------+--------------------+-------------------+--------------------+\n",
      "|         id|            location|sampling_rate|              sensor|          timestamp|    sensordatavalues|\n",
      "+-----------+--------------------+-------------+--------------------+-------------------+--------------------+\n",
      "|12819615587|{365.6, DE, 0, 16...|         null|{92, 1, {14, Nova...|2022-11-04 12:03:40|[{28631854905, 4....|\n",
      "|12815343559|{373.1, DE, 0, 49...|         null|{107, 5, {1, Shin...|2022-11-04 04:27:31|[{28621987635, 18...|\n",
      "|12819622473|{282.5, DE, 0, 65...|         null|{140, 1, {14, Nov...|2022-11-04 12:04:18|[{28631870936, 6....|\n",
      "|12819625695|{113.2, DE, 0, 63...|         null|{142, 1, {14, Nov...|2022-11-04 12:04:44|[{28631878373, 14...|\n",
      "|12819592483|{310.1, DE, 0, 45...|         null|{146, 1, {14, Nov...|2022-11-04 12:00:21|[{28631801861, 4....|\n",
      "+-----------+--------------------+-------------+--------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Raw dataset count:  25260\n",
      "Dataset count after exploding sensordatavalues:  62760\n",
      "Dataset count after removing rows that aren't P1 or P2:  26332\n",
      "Dataset count after joining the new sensordatavalues column to the old dataframe:  13186\n",
      "+-----------+--------------------+-------------+--------------------+-------------------+--------------------+\n",
      "|         id|            location|sampling_rate|              sensor|          timestamp|    sensordatavalues|\n",
      "+-----------+--------------------+-------------+--------------------+-------------------+--------------------+\n",
      "|12832804541|{365.6, DE, 0, 16...|         null|{92, 1, {14, Nova...|2022-11-05 11:59:48|[{28662311763, 10...|\n",
      "|12829426514|{373.1, DE, 0, 49...|         null|{107, 5, {1, Shin...|2022-11-05 06:08:21|[{28654510401, 11...|\n",
      "|12832787813|{282.5, DE, 0, 65...|         null|{140, 1, {14, Nov...|2022-11-05 11:57:52|[{28662273062, 10...|\n",
      "|12832807814|{113.2, DE, 0, 63...|         null|{142, 1, {14, Nov...|2022-11-05 12:00:09|[{28662319347, 13...|\n",
      "|12832800093|{310.1, DE, 0, 45...|         null|{146, 1, {14, Nov...|2022-11-05 11:59:15|[{28662301265, 6....|\n",
      "+-----------+--------------------+-------------+--------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing (P1 and P2 filtering):\n",
    "for i in range(len(dfs)):\n",
    "    print(\"Raw dataset count: \", dfs[i].count())\n",
    "    # Explode sensordatavalues using pyspark.sql.functions.explode\n",
    "    df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    print(\"Dataset count after exploding sensordatavalues: \", df_.count())\n",
    "    # Remove rows that aren't P1 or P2:\n",
    "    df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "    print(\"Dataset count after removing rows that aren't P1 or P2: \", df_.count())\n",
    "    # Regroup sensordatavalues by record id:\n",
    "    df_ = df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Remove the old sensordatavalues column still containing values different from P1 and P2:\n",
    "    dfs[i] = dfs[i].drop('sensordatavalues')\n",
    "    # Link the new sensordatavalues column to the old dataframe, on id:\n",
    "    dfs[i] = dfs[i].join(df_, on='id', how='inner')\n",
    "    print(\"Dataset count after joining the new sensordatavalues column to the old dataframe: \", dfs[i].count())\n",
    "\n",
    "    dfs[i].show(5)\n",
    "    # dfs[i].select('id','location.country','location.id', 'sensordatavalues.value_type','sensordatavalues.value').sort('location.country', 'location.id').show(5, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+-----+-----+------+------+------+\n",
      "|location|    sensordatavalues|                  P1|                  P2|avgP1|avgP1|P1_AQI|P2_AQI|maxAQI|\n",
      "+--------+--------------------+--------------------+--------------------+-----+-----+------+------+------+\n",
      "|      AD|[{28512148628, 14...|              [14.6]|               [5.5]| 14.6|  5.5|     2|     1|     2|\n",
      "|      AL|[{28501540558, 18...|[18.78, 29.38, 54...|[9.62, 13.22, 24....|33.74|15.87|     3|     2|     3|\n",
      "|      AR|[{28511988059, 3....|[3.38, 15.23, 17....|[2.38, 2.58, 2.77...|10.99| 2.68|     1|     1|     1|\n",
      "|      AT|[{28483617772, 7....|[7.91, 7.32, 0.39...|[4.67, 3.27, 0.39...|40.52|19.83|     4|     2|     4|\n",
      "|      AU|[{28496018035, 0....|[0.24, 3.94, 1.27...|[0.15, 0.59, 0.34...| 3.09|  1.1|     1|     1|     1|\n",
      "|      AX|[{28512155019, 9....|        [9.36, 29.9]|        [5.66, 13.4]|19.63| 9.53|     2|     1|     2|\n",
      "|      AZ|[{28512185217, 23...|             [23.05]|             [11.02]|23.05|11.02|     2|     1|     2|\n",
      "|      BA|[{28508810882, 49...|[49.28, 123.33, 9...|[37.54, 61.81, 8....|49.04|30.11|     6|     3|     6|\n",
      "|      BE|[{28496681971, 22...|[22.1, 7.81, 7.45...|[12.21, 2.99, 3.9...|51.66|26.66|     6|     3|     6|\n",
      "|      BG|[{28487597903, 10...|[10.21, 13.59, 14...|[5.47, 4.33, 10.2...|24.13|13.08|     3|     2|     3|\n",
      "+--------+--------------------+--------------------+--------------------+-----+-----+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+--------------------+--------------------+--------------------+-----+-----+------+------+------+\n",
      "|location|    sensordatavalues|                  P1|                  P2|avgP1|avgP1|P1_AQI|P2_AQI|maxAQI|\n",
      "+--------+--------------------+--------------------+--------------------+-----+-----+------+------+------+\n",
      "|      AD|[{28543223032, 7....|              [7.79]|              [2.72]| 7.79| 2.72|     1|     1|     1|\n",
      "|      AL|[{28537095549, 25...|[25.94, 85.47, 15...|[13.39, 36.14, 5....|42.74|18.16|     5|     2|     5|\n",
      "|      AR|[{28543217657, 5....|[5.65, 3.51, 128....|[5.45, 1.35, 13.1...|18.56| 4.39|     2|     1|     2|\n",
      "|      AT|[{28514910994, 2....|[2.1, 218.26, 3.6...|[1.18, 59.88, 1.2...|41.11|20.03|     4|     2|     4|\n",
      "|      AU|[{28531217358, 2....|[2.4, 0.4, 0.38, ...|[0.38, 0.06, 0.13...| 1.96| 0.92|     1|     1|     1|\n",
      "|      AX|[{28543257071, 10...|      [10.21, 27.48]|       [5.15, 12.38]|18.84| 8.76|     2|     1|     2|\n",
      "|      AZ|[{28543247738, 20...|             [20.48]|              [8.97]|20.48| 8.97|     2|     1|     2|\n",
      "|      BA|[{28539378642, 39...|[39.26, 59.54, 91...|[28.58, 44.82, 68...|50.52|29.66|     6|     3|     6|\n",
      "|      BE|[{28514539090, 34...|[34.76, 16.03, 4....|[15.24, 5.78, 3.4...|45.42|21.78|     5|     2|     5|\n",
      "|      BG|[{28514563470, 32...|[32.56, 18.81, 5....|[16.85, 8.74, 3.9...|30.15|15.64|     3|     2|     3|\n",
      "+--------+--------------------+--------------------+--------------------+-----+-----+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dfs)):\n",
    "    # replace location field with country field:\n",
    "    df_ = dfs[i].withColumn('location', dfs[i]['location.country'])\n",
    "    # sort by country:\n",
    "    df_ = df_.sort('location')\n",
    "    # explode sensordatavalues:\n",
    "    df_ = df_.withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    # group by country:\n",
    "    df_ = df_.groupby('location').agg(collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Create a RDD collection of tuples (country, (P1, P2)), so that each rdd element is a combo of a country and either its P1 or P2 values. Then convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [float(y['value']) for y in x[1] if y['value_type'] == 'P1'])).toDF(['location', 'P1']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [float(y['value']) for y in x[1] if y['value_type'] == 'P2'])).toDF(['location', 'P2']), on='location', how='inner')\n",
    "    # Create a RDD collection to calculate the average P1 and P2 values for each country, and convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], float(np.round(np.mean(x[2]), 2)))).toDF(['location', 'avgP1']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], float(np.round(np.mean(x[3]), 2)))).toDF(['location', 'avgP1']), on='location', how='inner')\n",
    "    # Associate P1 and P2 avg to their respective AQI:\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[4]) <= air[y][2][1]][0])).toDF(['location', 'P1_AQI']), on='location', how='inner')\n",
    "    df_ = df_.join(df_.rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[5]) <= air[y][2][1]][0])).toDF(['location', 'P2_AQI']), on='location', how='inner')\n",
    "    # Calculate the max AQI for each country:\n",
    "    df_ = df_.withColumn('maxAQI', when(df_.P1_AQI > df_.P2_AQI, df_.P1_AQI).otherwise(df_.P2_AQI))\n",
    "\n",
    "    dfs[i] = df_\n",
    "    dfs[i].show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|location|diffAQI|\n",
      "+--------+-------+\n",
      "|      LU|     -3|\n",
      "|      ZA|     -2|\n",
      "|      TH|     -2|\n",
      "|      SE|     -2|\n",
      "|      IS|     -1|\n",
      "|      AD|     -1|\n",
      "|      BE|     -1|\n",
      "|      KH|     -1|\n",
      "|      HK|     -1|\n",
      "|      HR|      0|\n",
      "|      PT|      0|\n",
      "|      SA|      0|\n",
      "+--------+-------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Identify the top 10 countries in terms of average air quality improvement over the previous 24 hours as well as the current averaged air quality indices of each. As far as possible use the country field in the sensor data to identify the country.\n",
    "df1 = dfs[0].select('location', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "df2 = dfs[1].select('location', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "df_diff = df1.join(df2, on='location', how='inner')\n",
    "df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('location', 'diffAQI')\n",
    "df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "df_diff.show(12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2: Using the geo-coordinates from the sensor data, group the data into smaller regions using an appropriate clustering algorithm. Then determine the top 50 regions in terms of air quality improvement over the previous 24 hours."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 19:02:51 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------+\n",
      "|cluster_id|  cluster_center|sensor_amount|maxAQI|\n",
      "+----------+----------------+-------------+------+\n",
      "|         0|   [51.97, 5.93]|          295|     3|\n",
      "|         1|[-33.73, 151.09]|           42|     1|\n",
      "|         2|[51.27, -115.24]|            5|     1|\n",
      "|         3|  [43.12, 27.81]|           75|     3|\n",
      "|         4| [39.96, -75.88]|           11|     1|\n",
      "|         5|  [45.83, 11.96]|           88|     8|\n",
      "|         6|  [55.18, 73.42]|           56|     2|\n",
      "|         7|  [49.58, 11.05]|          172|     2|\n",
      "|         8|  [51.08, 17.16]|          192|     3|\n",
      "|         9|  [53.37, -6.32]|           39|     2|\n",
      "+----------+----------------+-------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------+\n",
      "|cluster_id|  cluster_center|sensor_amount|maxAQI|\n",
      "+----------+----------------+-------------+------+\n",
      "|         0|   [51.97, 5.93]|          299|     3|\n",
      "|         1|[-33.73, 151.09]|           42|     1|\n",
      "|         2|[51.27, -115.24]|            5|     1|\n",
      "|         3|  [43.12, 27.81]|           73|     2|\n",
      "|         4| [39.96, -75.88]|           11|     1|\n",
      "|         5|  [45.83, 11.96]|           88|     4|\n",
      "|         6|  [55.18, 73.42]|           56|     1|\n",
      "|         7|  [49.58, 11.05]|          170|     2|\n",
      "|         8|  [51.08, 17.16]|          198|     3|\n",
      "|         9|  [53.37, -6.32]|           39|     2|\n",
      "+----------+----------------+-------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------+\n",
      "|cluster_id|  cluster_center|sensor_amount|maxAQI|\n",
      "+----------+----------------+-------------+------+\n",
      "|         0|   [51.97, 5.93]|          294|     3|\n",
      "|         1|[-33.73, 151.09]|           41|     1|\n",
      "|         2|[51.27, -115.24]|            5|     1|\n",
      "|         3|  [43.12, 27.81]|           73|     4|\n",
      "|         4| [39.96, -75.88]|           11|     1|\n",
      "|         5|  [45.83, 11.96]|           87|     1|\n",
      "|         6|  [55.18, 73.42]|           54|     1|\n",
      "|         7|  [49.58, 11.05]|          177|     2|\n",
      "|         8|  [51.08, 17.16]|          199|     3|\n",
      "|         9|  [53.37, -6.32]|           40|     2|\n",
      "+----------+----------------+-------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1205:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------+\n",
      "|cluster_id|  cluster_center|sensor_amount|maxAQI|\n",
      "+----------+----------------+-------------+------+\n",
      "|         0|   [51.97, 5.93]|          300|     2|\n",
      "|         1|[-33.73, 151.09]|           45|     2|\n",
      "|         2|[51.27, -115.24]|            5|     1|\n",
      "|         3|  [43.12, 27.81]|           73|     5|\n",
      "|         4| [39.96, -75.88]|           11|     1|\n",
      "|         5|  [45.83, 11.96]|           85|     1|\n",
      "|         6|  [55.18, 73.42]|           54|     1|\n",
      "|         7|  [49.58, 11.05]|          178|     3|\n",
      "|         8|  [51.08, 17.16]|          198|     5|\n",
      "|         9|  [53.37, -6.32]|           40|     2|\n",
      "+----------+----------------+-------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    # Same process as task 1, we filter the dataframe to keep only P1 and P2 values\n",
    "    df_ = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    df_ = df_[df_.sensordatavalues.value_type.isin(['P1', 'P2'])]\n",
    "    dfs[i] = dfs[i].drop('sensordatavalues').join(df_.groupby('id').agg(collect_list('sensordatavalues').alias('sensordatavalues')), on='id', how='inner')\n",
    "    dfs[i] = dfs[i].select('sensor.id','location.latitude','location.longitude', 'location.altitude', 'sensordatavalues').sort('sensor.id')\n",
    "\n",
    "# Create a RDD with the sensor id, and a tuple of the latitude and longitude, using the oldest dataframe:\n",
    "rdd = dfs[0].rdd.map(lambda x: (x[0], (float(x[1]), float(x[2]))))\n",
    "# Create a KMeans model with 200 clusters using latitude and longitude values from the first dataframe:\n",
    "model = KMeans.train(rdd.map(lambda x: x[1]), 200, maxIterations=10, initializationMode=\"k-means||\", seed=23)\n",
    "# (This model will be used to predict the cluster for each sensor id in the second dataframe.)\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    # Create a RDD with the sensor id, and a tuple of the latitude and longitude using the current dataframe:\n",
    "    rdd = dfs[i].rdd.map(lambda x: (x[0], (float(x[1]), float(x[2]))))\n",
    "    # Create a RDD collection with both the sensor id and the corresponding predicted cluster:\n",
    "    rdd_clusters = rdd.map(lambda x: (x[0], model.predict(x[1])))\n",
    "    # Create a RDD containing the amount of sensors in each cluster:\n",
    "    rdd_sensorAmountByCluster = rdd_clusters.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "    # Store each cluster center in a dictionary (Each cluster center is a tuple of latitude and longitude), rounded to 2 decimals:\n",
    "    centers = {i: np.round(np.array(model.clusterCenters[i]), 2) for i in range(len(model.clusterCenters))}\n",
    "    # Add the cluster center to the RDD collection:\n",
    "    rdd_clusters = rdd_clusters.map(lambda x: (x[1], x[0], centers[x[1]].tolist()))\n",
    "    # Convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(rdd_clusters.toDF(['cluster_id', 'id', 'cluster_center']), on='id', how='inner')\n",
    "    dfs[i] = dfs[i].join(rdd_sensorAmountByCluster.toDF(['cluster_id', 'sensor_amount']), on='cluster_id', how='inner')\n",
    "    # Group by cluster, keeping the cluster center and the sensordatavalues:\n",
    "    dfs[i] = dfs[i].groupby('cluster_id').agg(collect_list('cluster_center')[0].alias('cluster_center'), collect_list('sensor_amount')[0].alias('sensor_amount'), collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # PROBLEM: sensordatavalues is a list of list of data values. To fix this, explode two times the sensordatavalues column:\n",
    "    dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    dfs[i] = dfs[i].withColumn('sensordatavalues', explode('sensordatavalues'))\n",
    "    # Then regroup by cluster, keeping the cluster center and the sensordatavalues for each cluster id:\n",
    "    dfs[i] = dfs[i].groupby('cluster_id').agg(collect_list('cluster_center')[0].alias('cluster_center'), collect_list('sensor_amount')[0].alias('sensor_amount'), collect_list('sensordatavalues').alias('sensordatavalues'))\n",
    "    # Once again, just like task 1, we compute the max AQI for each cluster:\n",
    "    # Create a RDD collection of tuples (country, (P1, P2)), so that each rdd element is a combo of a cluster and either its P1 or P2 values. Then convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [float(y['value']) for y in x[3] if y[2] == 'P1'])).toDF(['cluster_id', 'P1']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [float(y['value']) for y in x[3] if y[2] == 'P2'])).toDF(['cluster_id', 'P2']), on='cluster_id', how='inner')\n",
    "    # Create a RDD collection to calculate the average P1 and P2 values for each cluster, and convert RDDs to DataFrames and join them to the original dataframe:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], float(np.round(np.mean(x[4]), 2)))).toDF(['cluster_id', 'avgP1']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], float(np.round(np.mean(x[5]), 2)))).toDF(['cluster_id', 'avgP1']), on='cluster_id', how='inner')\n",
    "    # Associate P1 and P2 avg to their respective AQI:\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[6]) <= air[y][2][1]][0])).toDF(['cluster_id', 'P1_AQI']), on='cluster_id', how='inner')\n",
    "    dfs[i] = dfs[i].join(dfs[i].rdd.map(lambda x: (x[0], [y for y in air if air[y][2][0] <= np.round(x[7]) <= air[y][2][1]][0])).toDF(['cluster_id', 'P2_AQI']), on='cluster_id', how='inner')\n",
    "    # Calculate the max AQI for each cluster:\n",
    "    dfs[i] = dfs[i].withColumn('maxAQI', when(dfs[i].P1_AQI > dfs[i].P2_AQI, dfs[i].P1_AQI).otherwise(dfs[i].P2_AQI))\n",
    "    dfs[i] = dfs[i].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI')\n",
    "    dfs[i].show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1326:>               (0 + 1) / 1][Stage 1351:>               (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+-------+\n",
      "|cluster_id|  cluster_center|sensor_amount|diffAQI|\n",
      "+----------+----------------+-------------+-------+\n",
      "|        21|  [55.05, 85.56]|            5|     -7|\n",
      "|        84| [52.01, 112.32]|            6|     -4|\n",
      "|       132|   [53.9, 12.06]|           47|     -4|\n",
      "|        88|  [6.38, -10.78]|            2|     -2|\n",
      "|       165|  [39.18, 22.65]|           15|     -2|\n",
      "|        37|   [47.4, 19.07]|          219|     -2|\n",
      "|       196|  [45.13, 39.23]|           17|     -2|\n",
      "|        31|   [50.92, 6.97]|          398|     -2|\n",
      "|        36|   [50.11, 8.67]|          267|     -1|\n",
      "|        11|[-36.94, -64.18]|           10|     -1|\n",
      "|       103|  [27.67, 85.32]|            2|     -1|\n",
      "|       133|   [50.08, 7.94]|          139|     -1|\n",
      "|        42|   [32.7, 73.81]|            3|     -1|\n",
      "|       179|   [43.48, 1.52]|           41|     -1|\n",
      "|        46|   [52.2, 10.08]|          196|     -1|\n",
      "|         0|   [51.97, 5.93]|          294|     -1|\n",
      "|        25|  [48.27, -3.95]|           39|     -1|\n",
      "|        35|   [51.01, 3.52]|          144|     -1|\n",
      "|       106|  [40.64, 17.93]|           31|     -1|\n",
      "|       141| [39.77, -86.51]|            4|     -1|\n",
      "|       154|  [48.54, 35.42]|           14|     -1|\n",
      "|       157|[32.72, -116.05]|            7|     -1|\n",
      "|       184|  [46.61, 23.98]|           33|     -1|\n",
      "|        48|  [67.09, 26.23]|            3|     -1|\n",
      "|        58|   [52.73, -9.0]|            5|     -1|\n",
      "|        68|   [50.93, 4.45]|          439|     -1|\n",
      "|        77|   [51.52, 7.05]|          442|     -1|\n",
      "|        82|  [51.82, 19.56]|           52|     -1|\n",
      "|        85|   [46.9, 27.95]|           23|     -1|\n",
      "|        87|  [51.26, 12.17]|          192|     -1|\n",
      "|        92|    [48.75, 2.2]|          115|     -1|\n",
      "|       102|   [51.92, 8.45]|          195|     -1|\n",
      "|        33| [-33.79, 18.76]|            1|     -1|\n",
      "|        64|  [55.58, 61.18]|            8|      0|\n",
      "|        65| [-1.76, 110.04]|            1|      0|\n",
      "|         6|  [55.18, 73.42]|           54|      0|\n",
      "|        51|  [45.01, -0.62]|           15|      0|\n",
      "|        27|   [-1.3, 36.76]|            1|      0|\n",
      "|        73| [46.6, -122.74]|           17|      0|\n",
      "|        23|   [49.22, 8.48]|          280|      0|\n",
      "|        67|   [50.04, 19.2]|          247|      0|\n",
      "|         4| [39.96, -75.88]|           11|      0|\n",
      "|        39|  [43.09, -0.23]|           14|      0|\n",
      "|       104|  [64.25, 20.73]|            4|      0|\n",
      "|        29|  [52.54, 13.43]|          393|      0|\n",
      "|        71|  [33.1, -96.48]|            7|      0|\n",
      "|         5|  [45.83, 11.96]|           87|      0|\n",
      "|        75| [64.14, -21.88]|            3|      0|\n",
      "|        24|  [50.51, 22.15]|           49|      0|\n",
      "|        13|  [53.69, 10.07]|          398|      0|\n",
      "+----------+----------------+-------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = dfs[len(dfs)-2].select('cluster_id', 'cluster_center', 'sensor_amount', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_1')\n",
    "df2 = dfs[len(dfs)-1].select('cluster_id', 'maxAQI').withColumnRenamed('maxAQI', 'maxAQI_2')\n",
    "df_diff = df1.join(df2, on='cluster_id', how='inner')\n",
    "df_diff = df_diff.withColumn('diffAQI', df_diff.maxAQI_2 - df_diff.maxAQI_1).select('cluster_id', 'cluster_center', 'sensor_amount', 'diffAQI')\n",
    "df_diff = df_diff.sort('diffAQI', ascending=True)\n",
    "df_diff.show(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3: Calculate the longest streaks of good air quality (ie low index values) and display as a histogram.\n",
    "\n",
    "Assume task 2 is already done. We will use the dataframes where AQI values are already calculated for each cluster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------+\n",
      "|cluster_id|  cluster_center|sensor_amount|maxAQI|\n",
      "+----------+----------------+-------------+------+\n",
      "|         0|   [51.97, 5.93]|          295|     3|\n",
      "|         1|[-33.73, 151.09]|           42|     1|\n",
      "+----------+----------------+-------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------+\n",
      "|cluster_id|  cluster_center|sensor_amount|maxAQI|\n",
      "+----------+----------------+-------------+------+\n",
      "|         0|   [51.97, 5.93]|          299|     3|\n",
      "|         1|[-33.73, 151.09]|           42|     1|\n",
      "+----------+----------------+-------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------+\n",
      "|cluster_id|  cluster_center|sensor_amount|maxAQI|\n",
      "+----------+----------------+-------------+------+\n",
      "|         0|   [51.97, 5.93]|          294|     3|\n",
      "|         1|[-33.73, 151.09]|           41|     1|\n",
      "+----------+----------------+-------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------+\n",
      "|cluster_id|  cluster_center|sensor_amount|maxAQI|\n",
      "+----------+----------------+-------------+------+\n",
      "|         0|   [51.97, 5.93]|          300|     2|\n",
      "|         1|[-33.73, 151.09]|           45|     2|\n",
      "+----------+----------------+-------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+----------+\n",
      "|cluster_id|     streaks|current_streak|max_streak|\n",
      "+----------+------------+--------------+----------+\n",
      "|         0|[0, 0, 0, 0]|             4|         4|\n",
      "|         1|[0, 0, 0, 0]|             4|         4|\n",
      "|         2|[0, 0, 0, 0]|             4|         4|\n",
      "|         3|[0, 0, 1, 1]|             0|         2|\n",
      "|         4|[0, 0, 0, 0]|             4|         4|\n",
      "+----------+------------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The clusters ids with the longest streaks of good air quality (ie low index values), as well as the amount of clusters with that streak:\n",
      "+----------+--------------+--------------------+\n",
      "|max_streak|cluster_amount|         cluster_ids|\n",
      "+----------+--------------+--------------------+\n",
      "|         4|           119|[26, 65, 191, 19,...|\n",
      "|         3|            14|[22, 181, 161, 56...|\n",
      "|         2|            20|[77, 87, 79, 170,...|\n",
      "|         1|             9|[198, 84, 103, 28...|\n",
      "|         0|            38|[29, 112, 155, 19...|\n",
      "+----------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify task 2 is done:\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i].show(2)\n",
    "\n",
    "# Task 3: Calculate the longest streaks of good air quality (ie low index values) and display as a histogram.\n",
    "rdd = dfs[0].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1 ]))\n",
    "rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][0] == 0 else 0), (1 if x[1][0] == 0 else 0)))\n",
    "df = rdd_streaks.toDF(['cluster_id', 'streaks', 'current_streak', 'max_streak'])\n",
    "for i in range(1, len(dfs)):\n",
    "    rdd = dfs[i].rdd.map(lambda x: (x[0], [0 if x[3] < 4 else 1 ]))\n",
    "    rdd_streaks = rdd.map(lambda x: (x[0], x[1], (1 if x[1][len(x[1])-1] == 0 else 0)))\n",
    "    df = df.join(rdd_streaks.toDF(['cluster_id', 'streak', 'previous_streak']), on='cluster_id', how='inner')\n",
    "    df = df.withColumn('streaks', concat('streaks', 'streak'))\n",
    "    df = df.drop('streak')\n",
    "    df = df.withColumn('current_streak', when(df.previous_streak == 1, df.current_streak + 1).otherwise(0))\n",
    "    df = df.withColumn('max_streak', when(df.current_streak > df.max_streak, df.current_streak).otherwise(df.max_streak))\n",
    "    df = df.drop('previous_streak')\n",
    "df.sort('cluster_id').show(5)\n",
    "# Group by max_streak, show the cluster_id and the amount of clusters with that max_streak:\n",
    "df = df.groupBy('max_streak').agg(count('cluster_id').alias('cluster_amount'), collect_list('cluster_id').alias('cluster_ids'))\n",
    "df = df.sort('max_streak', ascending=False)\n",
    "print('The clusters ids with the longest streaks of good air quality (ie low index values), as well as the amount of clusters with that streak:')\n",
    "df.show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}